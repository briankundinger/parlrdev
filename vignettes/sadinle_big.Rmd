---
title: "sadinle_big"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{sadinle_big}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(devtools)
load_all()
library(parlrdev)
library(tidyverse)
```

```{r}
files <- list.files(path = "../data/SimulationDatafiles", full.names = T)
number_of_files <- 40
all_patterns = TRUE
R = NULL
n1 <- 500
n2 <- 500
overlap <- n1/2

ExtractRecords <- function(simulation, dataset, overlap, n1, n2){
  records <- read_csv(simulation, col_types = cols())

records$file <- rep(2:1, length.out = dim(records)[1])

records <- records %>% 
  janitor::clean_names() %>% 
  mutate(rec_id = as.numeric(str_extract(rec_id, "\\d{3}")) + 1)

# Ztrue <- rep(n1 + 1, n2)
# Ztrue[1:overlap] <- 1:overlap

if (dataset == 1){
records %>% 
  filter(file == dataset,
         rec_id <= n1) %>% 
  select(-rec_id) %>% 
  as.matrix(.) %>% 
  data.frame(.)
} else {
records %>% 
  filter(file == 2,
         rec_id %in% c(1:overlap, (n1 +1):(1000 - overlap))) %>% 
  select(-rec_id) %>% 
  as.matrix() %>% 
  data.frame(.)
}
}

big_file1 <- lapply(files[1:number_of_files], function(x){
  ExtractRecords(x, 1, overlap, n1, n2)
}) %>% 
  do.call(rbind, .)

big_file2 <- lapply(files[1:number_of_files], function(x){
  ExtractRecords(x, 2, overlap, n1, n2)
}) %>% 
  do.call(rbind, .)

nA <- nrow(big_file1)
nB <- nrow(big_file2)
Ztrue <- seq_len(nA)
nonmatches <- c(rep(F, overlap), rep(T, n1 - overlap)) %>% 
  rep(., number_of_files)
Ztrue[nonmatches] <- nA + 1

big_cd <- BRL::compareRecords(big_file1, 
                         big_file2, 
                         c(2, 3, 4, 5), 
                         types = c("lv", "lv", "bi", "bi"))

```


```{r}
# chunks_df1 <- 40
# chunk_size_df1 <- ceiling(dim(big_file1)[1]/chunks_df1)
# file1_chunk_id <- rep(1:chunks_df1, each = chunk_size_df1)[1:dim(big_file1)[1]]
# file1_partition <- split(big_file1, file1_chunk_id)


chunks_df2 <- 20
chunk_size_df2 <- ceiling(dim(big_file2)[1]/chunks_df2)
file2_chunk_id <- rep(1:chunks_df2, each = chunk_size_df2)[1:dim(big_file2)[1]]
file2_partition <- split(big_file2, file2_chunk_id)

ptm <- proc.time()

HashRecords <- function(df2){
  cd <- BRL::compareRecords(big_file1, 
                         df2, 
                         c(2, 3, 4, 5), 
                         types = c("lv", "lv", "bi", "bi"))
  
  cd[[1]] <- apply(cd[[1]], 2, as.numeric)
  patterns <- GetUniquePatterns2(cd, 
                                 R = 5, 
                                 all_patterns = TRUE)
  patterns
}

#Keep File A intact
patterns_hashed <- lapply(file2_partition, HashRecords)
cd <- SynthesizeHash(patterns_hashed)

# Chunk both files
# chunks_df1 <- 2
# chunks_df2 <- 2
# chunk_ids <- expand.grid(1:chunks_df1, 1:chunks_df2)
# chunk_size_df1 <- ceiling(dim(big_file1)[1]/chunks_df1)
# chunk_size_df2 <- ceiling(dim(big_file2)[1]/chunks_df2)
# file1_chunk_id <- rep(1:chunks_df1, each = chunk_size_df1)[1:dim(big_file1)[1]]
# file1_partition <- split(big_file1, file1_chunk_id)
# file2_chunk_id <- rep(1:chunks_df2, each = chunk_size_df2)[1:dim(big_file2)[1]]
# file2_partition <- split(big_file2, file2_chunk_id)
# 
# thing <- map2(chunk_ids[, 1], chunk_ids[, 2], ~
#        {df1 <- file1_partition[[.x]]
#        df2 <- file2_partition[[.y]]
#        df1_index <- which(file1_chunk_id == .x)
#        df2_index <- which(file2_chunk_id == .x)
#        HashRecords(df1, df2, df1_index, df2_index)
#        })

```


```{r}
library(parallel)

#Linux, for the cluster
# patterns_hashed <- parallel::mclapply(file2_partition, HashRecords, mc.cores = 1)
# cd <- SynthesizeHash(patterns_hashed)

```

```{r}
# Synthesize results with both chunked up
# thing2 <- lapply(seq_len(chunks_df1), function(J){
#   index <- which(chunk_ids[, 2] == J)
#   chunk_hashed <- thing[index]
# 
#   hash_list_big <- chunk_hashed %>%
#     map(`[[`, 4) %>%
#     flatten()
#   
#   counts_by_rec_big <-  map(hash_list_big, ~lapply(.x, length)) %>%
#     map(unlist) %>% 
#     map(~.x[-length(.x)])
#   
#   counts_big <- Reduce(`+`, counts_by_rec_big)
#   
#   patterns <- chunk_hashed[[1]][[1]]
#   
#   chunk_info <- list(patterns, 
#                      counts_big, 
#                      counts_by_rec_big, 
#                      hash_list_big)
#   chunk_info
# })
# 
# cd <- SynthesizeHash(thing2)
# View(thing3)
```

```{r}
# counts_big <- Reduce(`+`, map(patterns_hashed, ~.x[[2]]))
# 
# counts_per_rec_big <- patterns_hashed %>%
#     map(`[[`, 3) %>%
#     flatten()
# 
#   hash_list_big <- patterns_hashed %>%
#     map(`[[`, 4) %>%
#     flatten()
# 
#   comparisons <- list(indicators = NULL,
#                       n1 = dim(big_file2)[1],
#                       n2 = dim(big_file2)[1],
#                       nDisagLevs = c(4, 4, 2, 2),
#                       Ztrue = NULL)
# 
#   patterns <- list(patterns_hashed[[1]][[1]],
#        counts_big,
#        counts_per_rec_big,
#        hash_list_big)
# 
#   cd <- list(comparisons, patterns)
```

```{r}
chain <- BKSimple_hash2_big(cd)
Zchain <- chain$Z
Zhat <- LinkRecordsBK(Zchain, nA, 1, 1, 2, Inf)
Zhat_resolved <- ResolveConflicts(Zhat)
eval <- GetEvaluations(Zhat_resolved, Ztrue, nA)

elapsed <- proc.time() - ptm

object.size(cd) / (10^6)
eval
elapsed
```

```{r}
# HashRecords <- function(df1, df2, df1_index = NULL, df2_index = NULL){
#   cd <- CompareRecordsBK(df1, 
#                          df2, 
#                          c(2, 3, 4, 5), 
#                          types = c("lv", "lv", "bi", "bi"),
#                          df1_index = df1_index,
#                          df2_index = df2_index)
#   
#   cd[[1]] <- apply(cd[[1]], 2, as.numeric)
#   patterns <- GetUniquePatterns2(cd, 
#                                  R = 5, 
#                                  all_patterns = TRUE, 
#                                  df1_index = df1_index, 
#                                  df2_index = df2_index)
#   patterns
# }
```

