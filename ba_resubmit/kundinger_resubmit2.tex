\documentclass[ba]{imsart}
%
\pubyear{0000}
\volume{00}
\issue{0}
\doi{0000}
%\arxiv{}
\firstpage{1}
\lastpage{1}

\usepackage{lineno}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
\usepackage{graphicx}
\newtheorem{lemma}{Lemma}
\usepackage{booktabs}       % professional-quality tables
\usepackage{algorithm}      % algorithm environment
\usepackage{algpseudocode}
\usepackage{subcaption}

\startlocaldefs
\input{subfiles/preamble.tex}
\endlocaldefs

\begin{document}

%% *** Frontmatter *** 
\linenumbers
\begin{frontmatter}
\title{Efficient and Scalable Bipartite Matching with Fast Beta Linkage  (fabl)}
%\title{A Sample Document\thanksref{T1}}
%\thankstext{T1}{<thanks text>}
\runtitle{Efficient and Scalable Bipartite Matching with Fast Beta Linkage  (fabl)}

\begin{aug}
\author{\fnms{Brian} \snm{Kundinger}\thanksref{addr1}\ead[label=e1]{brian.kundinger@duke.edu}},
\author{\fnms{Jerome P.} \snm{Reiter}\thanksref{addr1}\ead[label=e2]{jreiter@duke.edu}}
\and
\author{\fnms{Rebecca C.} \snm{Steorts}\thanksref{addr2}\ead[label=e3]{beka@stat.duke.edu}}

\runauthor{Kundinger et al.}

\address[addr1]{Department of Statistical Science,
  Duke University,
  P.O.\ Box 90251,
  Durham, NC 27708, USA
  \printead{e1}, % print email address of "e1"
  \printead*{e2}
}

\address[addr2]{Departments of Statistical Science and Computer Science,
  Duke University,
  P.O.\ Box 90251,
  Durham, NC 27708, USA
  \printead{e3}
}

%\thankstext{<id>}{<text>}
\end{aug}

\begin{abstract}
%Recently, researchers have developed Bayesian versions of the Fellegi-Sunter model for record linkage. These have the crucial advantage of quantifying uncertainty from imperfect linkages.  
Within the field of record linkage, Bayesian methods have the crucial advantage of quantifying uncertainty from imperfect linkages. However, current implementations of Bayesian Fellegi-Sunter models are computationally intensive, making them challenging to use on larger-scale record linkage tasks. To address these computational considerations, we propose fast beta linkage (\texttt{fabl}), an extension to the Beta Record Linkage (\texttt{BRL}) method of \cite{sadinle_bayesian_2017}. Specifically, we use independent prior distributions over the matching space, allowing us to use hashing techniques that reduce computational overhead. This also allows us to complete pairwise record comparisons over large data files through parallel computing and to reduce memory costs through a new technique called storage efficient indexing. Through simulations and two case studies, we show that \texttt{fabl} has markedly increased speed with minimal loss of accuracy when compared to \texttt{BRL}.
%
%\textcolor{blue}{We propose a variation on Bayesian Fellegi-Sunter models that we call fast beta linkage, or \texttt{fabl}, and is motivated by computational considerations.  Our method extends that of \cite{sadinle_bayesian_2017}, who extended the early work of \cite{fellegi_theory_1969}.}

\end{abstract}

%% ** Keywords **
\begin{keyword}%[class=MSC]
\kwd{Bayesian methods}
\kwd{distributed computing}
\kwd{entity resolution}
\kwd{hashing}
\kwd{record linkage}
\end{keyword}

\end{frontmatter}




%% ** Mainmatter **


\section{Introduction}
\label{sec:introduction}

Before conducting data analysis, it is often necessary to identify duplicate records across two data files. This is an increasingly important task in ``data cleaning'' and is used for inferential and predictive analyses in fields such as statistics, computer science, machine learning, political science, economics, precision medicine, official statistics, and others \citep{christen_2012, gutman2013bayesian, DalzellReiter18, tang2020}. In this article, we consider bipartite record linkage, which merges two data files that contain duplications across, but not within, the respective data files. 

Many statistical record linkage methods are extensions of the seminal work of \cite{fellegi_theory_1969} and \cite{newcombe_automatic_1959}. Specifically, \cite{fellegi_theory_1969} created comparison vectors for each pair of records in the data files and independently classified each pair as a match or a non-match using a likelihood ratio test. Recent work in the statistical literature has extended this approach for a wide variety of applications \citep{Winkler1990, fair2004generalized, wagner2014person, gill2003english, enamorado2019using, aleshinguendel2021multifile}. Additionally, some methods directly model the linkage variables \citep{steorts_bayesian_2016, marchant_distributed_2019, betancourt2021prior}, but in this paper, we build on the contributions to the comparison vector approach. 

% Possible citation for bipartite matching. 
%\citep{fellegi_theory_1969, jaro1989, Winkler1988, belin_A995, larsen_2001, liseo_2011,  herzog2007data, gutman_bayesian_2013, sadinle_bayesian_2017}.

The independent pairwise matching assumption from \cite{fellegi_theory_1969} is popular mainly for its mathematical simplicity, but can be unreasonable in practice. In many situations, we know that there are no duplications within a data file, meaning that each record in one file should be linked with at most one other record in the other file. Thus, when the procedure results in many-to-one matches, some of these links must be false. Many extensions to \cite{fellegi_theory_1969} resolve these false matches as a post-processing step \citep{jaro1989}, but this model misspecification can still lead to poor results \citep{sadinle_bayesian_2017}.

Alternatively, one can embed one-to-one matching requirements into the model specification itself \citep{gutman2013bayesian, liseo_2011}, at an additional computational cost. \cite{Larsen05} employed a Metropolis-Hastings algorithm to only allow sampling matches that respected one-to-one assumptions, but such algorithms exhibit slow mixing due to the combinatorial nature of the constrained matching space. \cite{fortunato_2010} used simulated annealing to target the space of matches permitted under the one-to-one constraint, but the method is computationally intensive and, to our knowledge, has not been applied on data files with more than 100 records. \cite{sadinle_bayesian_2017} proposed the Beta Record Linkage model (\texttt{BRL}), using a prior distribution over the space of bipartite matchings to strictly enforce one-to-one requirements throughout a Gibbs sampler. Additionally, he introduced a class of loss functions that allows for a flexible estimation of the linkage structure, such that the modeler can weigh the relative importance of false positives and false negatives, and identify records pairings to be decided through clerical review. \texttt{BRL} was shown to work on larger tasks than previous one-to-one methods, but in our experience, it becomes slow when applied to files with more than a few thousand records. 

In this paper, we propose fast beta linkage (\texttt{fabl}), which extends the \texttt{BRL} model for increased efficiency and scalability. Following the suggestion in \cite{wortman2019}, we relax the one-to-one matching requirement of \texttt{BRL} and use independent priors over the matching space. This allows us to (1) employ hashing techniques that speed up calculations and reduce computational costs, (2) compute the pairwise record comparisons over large data files via parallel computing, and (3) reduce memory costs through what we call storage efficient indexing. These contributions allow \texttt{fabl} to perform record linkage on much larger data files than previous Bayesian Fellegi-Sunter models at significantly increased speed with minimal loss of accuracy. In particular, computation time under \texttt{BRL} grows quadratically, with the size of each data file, while computation time under \texttt{fabl} grows linearly, only with the size of the smaller data file.

%\textcolor{blue}{Moreever, we propose storage efficient indexing (SEI), which reduces memory costs.}
%\textcolor{blue}{More specifically, we use both hashing and SEI to increase the scale of the linkage tasks without the use of blocking. This can be useful when there is no reliable blocking field available or one desires estimates of model parameters for the entire sample in question. In practice, \texttt{fabl} can be combined with blocking, but all derivations, simulations, and case studies are presented here without blocking. We illustrate our methodology on two case studies and two simulation scenarios, where Open source software to use \texttt{fabl} in \texttt{R} is available through \href{https://github.com/briankundinger/parlrdev}{Github}.}

%We demonstrate \texttt{fabl} provides accurate estimation of the linkage structure and greatly enhanced speed. Open source software to use \texttt{fabl} in \texttt{R} is available through \href{https://github.com/briankundinger/parlrdev}{Github}.

In what follows, Section \ref{sec:review-of_prior-work} reviews the work of \cite{fellegi_theory_1969} and \cite{sadinle_bayesian_2017}. Section \ref{sec:fast-beta-linkage} proposes the \texttt{fabl} model, provides the Gibbs sampler for posterior inference, and shows the loss function used to calculate the Bayes estimate for the bipartite matching. Section \ref{sec:efficiency} introduces the hashing technique and storage efficient indexing used to increase the speed of calculations and the scale of linkage tasks amenable to \texttt{fabl}. Sections \ref{sec:simulations} and \ref{sec:case-studies} demonstrate the speed and accuracy of \texttt{fabl} through simulation studies and case studies of homicides from the El Salvadoran Civil War and the National Long Term Care Study. Finally, Section \ref{discussion} summarizes our contributions and highlights areas for further research.

\section{Review of Prior Work}
\label{sec:review-of_prior-work}

%Consider two data files $A$ and $B$ containing records $\{x_{1i}\}_{i=1}^{n_A}$ and $\{x_{2j}\}_{j=1}^{n_B}$ respectively.

Consider two data files $A$ and $B$, consisting of records $\{A_i: i = 1, \ldots, n_A\}$ and $\{B_j: j = 1, \ldots, n_B\}$. Suppose the files have $F$ fields in common to be used for linkage, and that these fields take on values $A_{if}$ and $B_{jf}$ respectively. Without loss of generality, denote files such that $n_A \geq n_B$. Under bipartite matching, the set of matches across data files can be represented in two equivalent ways. First, we may use a matrix $\Delta \in \{0, 1\}^{n_A \times n_B}$, where
\begin{align}
	\Delta_{ij} =
	\begin{cases}
		1, & \text{if records}\;  A_i \; \text{and}\; B_j \; \text{refer to the same entity}; \\
		0, & \text{otherwise}.\\
	\end{cases}
\end{align}
This sparse matrix representation can become cumbersome for large linkage tasks. More compactly, bipartite matching also can be viewed as a labeling $\bm{Z} = (Z_1, \ldots, Z_{n_B})$ for the records in $B$ such that 
\begin{align}
	Z_{j} =
	\begin{cases}
		i, & \text{if records}\;  A_i \; \text{and}\; B_j  \; \text{refer to the same entity}; \\
		n_A + j, & \text{if record}\;  B_j \; \text{does not have a match in}\; A. \\
	\end{cases}
\end{align}
We can go back and forth between the two using $\Delta_{ij} = I(Z_j = i),$ where $I(\cdot) = 1$ when the expression inside the parentheses is true, and $I(\cdot) = 0$ otherwise. 

{\color{red}{The following paragraph is pretty unnecessary, I never use this notation}}
Denote the set of matches by $\bm{M} = \{(i,j): i \leq n_A, j \leq n_B, \Delta_{ij} = 1\}$, and the set of non-matches by 
$\bm{U} =  \{(i,j): i \leq n_A, j \leq n_B, \Delta_{ij} = 0\}.$ The record linkage task can be viewed as identifying  $\bm{M}$ and  $\bm{U}.$ We refer to record pairs that are estimated as matches as ``links'' and to record pairs that are estimated as non-matches as ``non-links.''

%\subsection{Comparison Vectors}
%\label{comparison-vectors}

{\color{red}{Beka suggested eliminating the following two paragraphs. Thoughts? I think the first one is good, the second seems unneccesary.}}

Intuitively, matching records (those that refer to the same entity) should be similar; records that are non-matching should be dissimilar. \cite{fellegi_theory_1969} proposed encoding this using a comparison vector $\gamma_{ij}$ computed for each record pair $(i,j)$ in $A \times B.$ Denote the number of criteria for comparing records by $F$, such that $\gamma_{ij} = (\gamma_{ij}^1, \ldots, \gamma_{ij}^F).$ {\color{red}{We use $\Gamma_{ij}$ when referring to the vector as a random variable, and $\gamma_{ij}$ when referring to observed data. We collect the vectors $\gamma_{ij}$ as the comparison matrix $\gamma \in \mathbb{R}^{n_A n_B \times F}$}}.

%Frequently, $\gamma_{ij}$ consists of one comparison for each feature shared between the two data files. 

The simplest way to compare any particular feature for two records is to check for exact agreement, and this is commonly used for categorical features. For example, if zip code is linking feature $f$, we can set $\gamma_{ij}^f=1$ when the zip codes for records $A_i$ and $B_j$ agree exactly, and set $\gamma_{ij}^f=2$ when they do not. For numerical features, we can use absolute difference between the two feature values. For example, if age is linking field $f$, we can set $\gamma_{ij}^f = 1$ when the ages for records $A_i$ and $B_j$ match exactly, $\gamma_{ij}^f = 2$ when the ages for records $A_i$ and $B_j$ are within one year but not equal, and $\gamma_{ij}^f = 3$ when the ages are two or more years apart. For text features, such as names, we can use string distance metrics such as Levenstein or Jaro-Winkler distance \citep{cohen2003comparison}. We then set thresholds that allow us to represent comparisons through discrete levels of disagreement \citep{bilenko2006riddle, elmagarmid_duplicate_2007}. 

More generally, let $\mathcal{S}_f(i,j)$ denote a similarity measure for feature $f$ of records $A_i$ and $B_j,$ where the range of $\mathcal{S}_f$ can be divided into $L_f$ intervals denoted by $I_{f1}, \ldots, I_{fL_f}$. Here, $I_{f1}$ represents the highest level of agreement (including complete agreement) and $I_{fL_f}$ represents the highest level of disagreement (including complete disagreement). Thus, we can construct comparison vectors such that $\gamma_{ij}^f = l \; \text{if} \; \mathcal{S}_f(i,j) \in I_{fl}.$ The choices of $I_{fl}$ are application specific, as we discuss in the simulation and case studies.

{\color{red}{
In the construction of comparison vectors, it is common to encounter missing information in record $A_i$ or $B_j$. As a result, the comparison vector $\mathbf{\gamma}_{ij}$ will have missing values. We assume that this missingness occurs completely at random (MCAR, per \cite{LittleRubin2002}). To notate a missing value in any $\gamma_{ij}^f$, we use $I_{obs}(\gamma_{ij}^f)=1$ when $\gamma_{ij}^f$ is observed and $I_{obs}(\gamma_{ij}^f)=0$ otherwise. With the MCAR assumption, we can marginalize over the missing data, and do all computation simply using the observed data. }
}

%For more complex measurements, we can take into account partial agreement to more richly characterize the comparison;  

%
%In this paper, we use hashing and a new technique called storage efficient indexing to increase the scale of the linkage tasks we can undertake without blocking. This is useful when there is no reliable blocking field available, or one desires estimates of model parameters for the entire sample in question. In practice, \texttt{fabl} can be combined with blocking, but all derivations, simulations, and case studies are presented here without blocking. 

\subsection{Fellegi-Sunter Models}
\label{fellegi-sunter}

The seminal \cite{fellegi_theory_1969} model employs two independence assumptions: first, that comparison vectors are conditionally independent given their matching status, and second, that the matching status of the record pairs are independent. Using these assumptions, \cite{winkler_state_1999}, \cite{jaro1989}, \cite{larsen_2001}, \cite{enamorado2019using} and others model the comparison data through mixture models of the form
\begin{subequations}
\begin{align}
	&\Gamma_{ij} = \gamma_{ij} \mid \Delta_{ij} = 1 \stackrel{iid}{\sim} \mathcal{M}(\bm{m}), \label{eqn:fs_model} \\
	&\Gamma_{ij} = \gamma_{ij} \mid \Delta_{ij} = 0  \stackrel{iid}{\sim} \mathcal{U}(\bm{u}), \label{eqn:m_dist}\\
	&\Delta_{ij}   \stackrel{iid}{\sim} \text{Bernoulli}(\lambda). \label{eqn:u_dist}
\end{align}
\end{subequations}
Here, $\mathcal{M}$ and $\mathcal{U}$ are the distributions for matching and non-matching record pairs, $\bm{m}$ and $\bm{u}$ are their respective sets of parameters, and $\lambda$ is the marginal probability that a record pair is a match. When using comparison vectors with discrete agreement levels, $\mathcal{M}$ and $\mathcal{U}$ are collections of independent multinomial distributions for each linkage feature. Accordingly, $\bm{m} = (\bm{m}_1, \ldots, \bm{m}_F)$, where $\bm{m}_f = (m_{f1}, \ldots, m_{fL_f})$ and $m_{fl} = P(\gamma_{ij}^f = l|\Delta_{ij} = 1)$ for all fields $f$ and agreement levels $l$. The $\bm{u}$ parameters are defined similarly, with $u_{fl} = P(\gamma_{ij}^f = l|\Delta_{ij} = 0)$.

Within this framework, record pairs are independently classified as matches and non-matches based on the estimated parameters. However, such independent classifications often leads to links that violate one-on-one matching assumptions, requiring post-processing to achieve desirable results. To address this issue, \cite{jaro1989} proposed an optimization technique using the estimated model parameters to produce a bipartite matching. \cite{sadinle_bayesian_2017} later showed that this method is equivalent to a maximum likelihood estimate.

%This prompted \cite{jaro1989} to formulate an optimization problem that produces a bipartite matching maximizing the sum of the Fellegi-Sunter weights among matched pairs.

%These unknown parameters are often estimated using the EM algorithm \cite{winkler_state_1999}. 

%The Fellegi-Sunter method then uses thresholds $T_m$ and $T_u$ such that all record pairs with $w_{ij} > T_m$ are declared as links, and all those with $w_{ij} < T_u$ are declared as non-links (with those such that $T_u < w_{ij} < T_m$ left undetermined and subject to clerical review). This is a problem in practice as transitive closures can be violated. Since each $w_{ij}$ is calculated independently, there can be situations in which both $w_{ij} > T_m$ and $w_{i'j} > T_m$, so both $(i,j)$ and $(i', j)$ are declared as links, even though such a matching is not allowed by assumption. 

%To address this issue, \cite{jaro1989} proposed solving the following linear sum assignment problem:
%\begin{align}
%	\label{eqn:jaro}
%	&\max_{\Delta} \sum_{i=1}^{n_A} \sum_{j=1}^{n_B} w_{ij} \Delta_{ij} 
%	\quad \text{subject to} \quad \Delta_{ij} \in \{0,1\}; \\
%	& \quad \quad \sum_{i=1}^{n_A}  \Delta_{ij}  \leq 1, j=1,2, \ldots n_B; \; \text{and} \notag\\
%	& \quad \quad\sum_{i=1}^{n_B}  \Delta_{ij}  \leq 1, i=1,2, \ldots n_A. \notag
%\end{align}
%
%The solution to this problem, for which several simple algorithms exist, is a bipartite matching that maximizes the sum of the Fellegi-Sunter weights among matched pairs. Jaro provided no theoretical justification for using this approach; however \cite{sadinle_bayesian_2017} recently showed that under certain conditions, (\ref{eqn:jaro}) is the maximum likelihood estimate for bipartite matching. 

%The comparison data $\gamma_{ij}$ are not sufficient to determine whether a record is a match or non-match because of errors that naturally occur in the data. This motivated \cite{fellegi_theory_1969} to consider the likelihood ratio
%\begin{align}
%	\label{eqn:wts}
%	w_{ij} = \frac{P(\gamma_{ij} \mid \Delta_{ij} = 1)}{P(\gamma_{ij} \mid \Delta_{ij} = 0)}
%\end{align}
%as a weight to estimate if record pairs are a match or non-match. This ratio will be large when there is strong evidence of the pair being a match, and small otherwise. 
%
%In practice, $P(\cdot \mid \Delta_{ij} = 1)$ and $P(\cdot \mid \Delta_{ij} = 0)$ are unknown to the modeler, and thus must be estimated. 
%
%%is the set of parameters $u_{fl} = P(\gamma_{ij}^f = l|Z_j \neq i)$ for the non-matching pairs.
%
%After estimating $\bm{m}$ and $\bm{u}$, the Fellegi-Sunter method uses thresholds $T_m$ and $T_u$ such that all record pairs with $w_{ij} > T_m$ are declared as links, and all those with $w_{ij} < T_u$ are declared as non-links (with those such that $T_u < w_{ij} < T_m$ left undetermined and subject to clerical review). This is a problem in practice as transitive closures can be violated. Since each $w_{ij}$ is calculated independently, there can be situations in which both $w_{ij} > T_m$ and $w_{i'j} > T_m$, so both $(i,j)$ and $(i, j')$ are declared as links, even though such a matching is not allowed by assumption. 

%\subsection{Beta Record Linkage Model}
%\label{BRL}

%To estimate a one-to-one matching without using a post-processing step, \cite{sadinle_bayesian_2017} incorporated this constraint directly into the model through a prior distribution over the matching space. 

To estimate a one-to-one matching without using a post-processing step, \cite{sadinle_bayesian_2017} incorporates this constraint directly into the model through a Bayesian framework. In addition to {\color{red}{uniform}} Dirichlet priors for the $\bm{m}_f$ and $\bm{u}_f$ parameters, he proposes the ``beta distribution for bipartite matching" for the linkage parameter $\bm{Z}$. He assigns a prior distribution for the probability of the indicator that a record in $B$ has a match in $A$, so that $I(Z_j \leq n_A) \sim \text{Bernoulli}(\pi)$, where $\pi$ itself is taken to be distributed $\text{Beta}(\alpha_{\pi}, \beta_{\pi})$. It follows that the number of records in $B$ that have matches, denoted $n_{AB}(\bm{Z}) = \sum_{j=1}^{n_B} I(Z_j \leq n_A)$, is distributed according to a $\text{Beta-Binomial}(n_B, \alpha_{\pi}, \beta_{\pi})$. Conditioning on the set of records in $B$ that have matches, formally denoted $\{I(Z_j \leq n_A)\}_{j=1}^{n_B}$, all $n_A ! / (n_A - n_{AB}(\bm{Z}))!$ bipartite matchings are taken to be equally likely. Thus, the prior is given by
\begin{align}
\label{eqn:sadinle_prior}
P(\bm{Z}|\alpha_{\pi}, \beta_{\pi}) = \frac{(n_A - n_{AB}(\bm{Z}))!}{n_A !}\frac{\text{B}(n_{AB}(\bm{Z}) + \alpha_{\pi}, n_B - n_{AB}(\bm{Z}) + \beta_{\pi})}{\text{B}(\alpha_{\pi}, \beta_{\pi})},
\end{align}
where $\text{B}(\cdot, \cdot)$ represents the Beta function. This prior strictly enforces one-to-one matching, inducing a Gibbs sampler that removes previously matched records from the set of candidate records when sampling each $Z_j$. This makes the sampler inherently serial, which can be slow when working on linkage tasks with more than a few thousand records.

\section{Fast Beta Linkage}
\label{sec:fast-beta-linkage}

In contrast to the prior over the vector $\bm{Z}$ from \cite{sadinle_bayesian_2017}, we follow \cite{wortman2019} and use independent priors for each component $Z_j$. However, unlike \cite{wortman2019} who proposes a flat prior for $Z_j$, we use the fast Beta prior as follows. For each $Z_j$, we have
\begin{align}
	\label{eqn:fast_beta_prior}
	p(Z_j = q| \pi) &= \begin{cases} 
	\frac{1}{n_A}\pi,  & q \leq n_A; \\
	1-\pi, &  q  = n_A + j; \\
\end{cases} \\
\pi &\sim \text{Beta}(\alpha_{\pi}, \beta_{\pi}) \notag.
\end{align}
We can interpret (\ref{eqn:fast_beta_prior}) as follows: record $B_j$ has some match in $A$ with probability $\pi$, and each record $A_i$ is equally likely to be that match. The hyperparameters $\alpha_{\pi}$ and $\beta_{\pi}$ encode prior beliefs about  the proportion of records in $B$ that have matches in $A.$ 

In the \cite{wortman2019} flat prior, each value $\{1, \ldots, n_A, n_A +j\}$ is a priori equally likely for $Z_j$. This however amounts to a prior probability of $n_A / (n_A + 1)$ that record $B_j$ has a match in $A$. In our preliminary studies, the flat prior results in poor precision; hence, we prefer (\ref{eqn:fast_beta_prior}). We also note that the flat prior is equivalent to a special case of the fast Beta prior with $\pi$ fixed at the mean of a $\text{Beta}\left(1, 1 / n_A \right)$ random variable.

%Intuitively, this set of priors says that record $B_j$ has some match in $A$ with probability $\pi$, and that each record $A_i$ is equally likely to be that match. $\pi$ itself is given a prior distribution with hyperparameters $\alpha_{\pi}$ and $\beta_{\pi}$ to encode prior beliefs about  the proportion of records in $\bm{X_2}$ that have matches in $\bm{X_1}$. One non-informative choice might be $\pi \sim \text{Beta}(1, 1)$, which corresponds to a prior belief that non-matches and matches are equally likely, and another might be $\pi \sim \text{Beta}\left(1, \frac{1}{n_A}\right)$, which corresponds to a uniform prior on the labeling of $\bm{Z}$. We note here that the prior from \cite{wortman2019} can be viewed as a special case of the fast beta prior, with $\pi$ fixed at the mean of a $\text{Beta}\left(1, \frac{1}{n_A}\right)$ random variable. 

Note that linkage with \texttt{fabl} is conducted at the record level, rather than at the record pair level, as in the Fellegi-Sunter model. That is, $\pi$ under \texttt{fabl} estimates the proportion of records in $B$ that have matches, whereas $\lambda$ in the Fellegi-Sunter model estimates the proportion of record pairs that are matches. We find $\pi$ to be more a interpretable parameter than $\lambda$ in the bipartite case. In this setting, there are at most $n_B$ matching pairs out of $n_A n_B$ total pairs, meaning that $\lambda$ is bounded above by $\frac{1}{n_A}$ and tends towards 0 as the size of the linkage task grows. Additionally, while the Fellegi-Sunter model makes $n_A \times n_B$ independent matching decisions and \texttt{BRL} makes $n_B$ dependent matching decisions, \texttt{fabl} strikes a middle ground between the two, making $n_B$  independent matching decisions. As shown in Sections \ref{sec:simulations} and \ref{sec:case-studies}, this allows \texttt{fabl} to fit a Bayesian record linkage model like \texttt{BRL} while making computational efficiency gains possible by exploiting independence. 

To obtain an estimate $\hat{\bm{Z}}$ of the linkage structure, we use the loss functions and Bayes estimate from \cite{sadinle_bayesian_2017}. However, there is a crucial difference: since \ref{eqn:fast_beta_prior} does not strictly enforce one-to-one matching, it is possible for this Bayes estimate to link multiple records in $B$ to one record in $A$. To achieve a Bayes estimate that fulfills the one-to-one matching requirement, we minimize the expected loss subject to the constraint that $\hat{Z}_j \neq \hat{Z}_{j'}$ for all $j \neq j'$. Details for the initial Bayes estimate and the post-processing procedure are provided in Appendix \ref{bayes-estimate}.

\begin{table}[t!]
	\centering
	\begin{tabular}[t!]{ll}
		Symbol & Description \\
		\hline
		$A, B$ & data files\\
		$i \in 1, \ldots, n_A $ & index over records in $A$\\
		$j \in 1, \ldots, n_B $ & index over records in $B$\\
		$f \in 1, \ldots F$ & index over fields used for comparisons \\
		$l \in 1, \ldots L_f$ & index over agreement levels for feature $f$ \\
		$n_{AB}$ & number of entities in common between $A$ and $B$\\
		$\gamma_{ij}$ & comparison vector for records $A_i$ and $B_j$ \\
		$Z_j = i$ & records $A_i$ and $B_j$ match \\
		$Z_j = n_A + j$ & record $B_j$ has no match in $A$ \\
		$m_{fl}$ & $P(\gamma_{ij}^f = l | Z_j = i)$ \\
		$u_{fl}$ & $P(\gamma_{ij}^f = l | Z_j \neq i)$ \\
		$\pi$ & probability that a record in $B$ has a match in $A$ \\
		\hline
	\end{tabular}\caption{Summary of model notation.}\label{table_notation_A}
\end{table}

For clarity, we present our full model below. A summary of notation is provided in Table \ref{table_notation_A}.
\begin{subequations}
\begin{align}
	\mathcal{L}(\bm{Z}, \bm{m}, \bm{u} \mid \gamma) &= \prod_{i=1}^{n_A}  \prod_{j=1}^{n_B}\prod_{f=1}^{F}\prod_{l=1}^{L_f}\left[  m_{fl}^{I(Z_j = i)}u_{fl}^{I(Z_j \neq i)}\right]^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}, \label{eqn:likelihood}\\
	\bm{m}_f &\sim \text{Dirichlet}(\alpha_{f1}, \ldots, \alpha_{f L_f}), \forall f = 1, \ldots, F, \label{eqn:m} \\
	\bm{u}_f &\sim \text{Dirichlet}(\beta_{f1}, \ldots, \beta_{f L_f}),\forall f = 1, \ldots, F,  \label{eqn:u}\\
	p(Z_j = q| \pi)  &=
	\begin{cases} 
		\frac{1}{n_A}\pi,  & q \leq n_A; \\
		1-\pi, &  q  = n_A + j; \\
	\end{cases} \label{eqn:z}\\
	\pi &\sim \text{Beta}(\alpha_{\pi}, \beta_{\pi})\label{eqn:pi}.
\end{align}
\end{subequations}

\hypertarget{posterior-sampling}{%
	\subsection{Gibbs Sampler}
	\label{gibbs_sampling}}

We initialize $\bm{m}$ and $\bm{u}$ from random draws from their prior distributions, and initialize $\bm{Z}$ to reflect no matches across data files; that is, $\bm{Z} = (n_A + 1, \ldots, n_A + n_B)$. To take the $(s+1)$th sample of each $\bm{m}_f$ and $\bm{u}_f$ given $\bm{Z}^s$, we use the full conditionals,
\begin{subequations}
\begin{align}
	\bm{m}_f^{(s+1)}|\gamma, \bm{Z}^{(s)} &\sim \text{Dirichlet}(\alpha_{f1}(\bm{Z}^{(s)}), \ldots, \alpha_{fL_f}(\bm{Z}^{(s)})), \label{eqn:m_update} \\
	\bm{u}_f^{(s+1)}|\gamma, \bm{Z}^{(s)} &\sim \text{Dirichlet}(\beta_{f1}(\bm{Z}^{(s)}), \ldots, \beta_{fL_f}(\bm{Z}^{(s)})), \label{eqn:u_update} \\
	\text{where }\alpha_{fl}(\bm{Z}^{(s)})&= \alpha_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(Z_j^{(s)} = i), \label{eqn:alpha_update} \\
	\text{and } \beta_{fl}(\bm{Z}^{(s)})&=  \beta_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(Z_j^{(s)} \neq i) \label{eqn:beta_update}
	%where $\alpha_{fl}(\bm{Z}^{(s)})= \alpha_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(Z_j^{(s)} = i)$, and $\beta_{fl}(\bm{Z}^{(s)})= \\ \beta_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f) I(\gamma_{ij}^f = l) I(Z_j^{(s)} \neq i)$.
\end{align}
\end{subequations}
Next, the full conditional for $\pi$ is given by
\begin{align}
	\pi^{(s+1)}|\bm{Z}^{(s+1)},  \alpha_{\pi}, \beta_{\pi}, \pi \sim  \text{Beta}(n_{AB}(\bm{Z}) + \alpha_{\pi}, n_B - n_{AB}(\bm{Z}) + \beta_{\pi}).
\end{align}

Lastly, we sample $\bm{Z}$ componentwise from the full conditionals for each $Z_j$:
\begin{align}
	\label{eqn:z_full_conditional}
	p\left(Z_j^{(s+1)}  = q|\gamma, \bm{m}^{(s+1)}, \bm{u}^{(s+1)}, \bm{Z^{(s)}}\right) \propto
	\begin{cases} 
		\frac{\pi^{(s+1)}}{n_A} w_{q, j}^{(s+1)},  & q \leq n_A; \\
		1 - \pi^{(s+1)}, & q  = n_A + j, \\
	\end{cases}
\end{align}
%\begin{align}
%\label{eqn:z_full_conditional}
%p\left(Z_j^{(s+1)}  = z_j|\gamma, \bm{m}^{(s+1)}, \bm{u}^{(s+1)}, \bm{Z^{(s)}}\right) \propto
%\begin{cases} 
%	w_{z_j, j}^{(s+1)},  & z_j \leq n_A; \\
%	n_A \frac{n_B - n_{AB}(\bm{Z}^{(s)}) + \beta_{\pi}}{n_{AB}(\bm{Z}^{(s)}) + \alpha_{\pi}}, & z_j  = n_A + j, \\
%\end{cases}
%\end{align}
where, for all $i \in \{1, \ldots, n_A\}$, 
\begin{align}
	\label{eqn:fs_weight}
	w_{ij}^{(s)} = \prod_{f=1}^{F}\prod_{l = 1}^{L_f} \left(\frac{m_{fl}^{(s)}}{u_{fl}^{(s)}}\right)^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}.
\end{align}
Derivations for these full conditionals can be found in Appendix \ref{app:derivations}.

\section{Efficient and Scalable Implementation}
\label{sec:efficiency}

The scale of linkage tasks possible through \texttt{BRL} is limited by the memory costs of storing $n_A \times n_B$ comparison vectors for every pair of records across the two data files, and the speed of the linkage algorithm over those comparison vectors. One approach to reduce the number of comparisons is blocking, which places similar records into partitions, or ``blocks'' \citep{christen2019data}. In deterministic blocking, the modeler chooses fields thought to be highly reliable, and only compares records that agree on those fields. The record linkage method is  applied independently across all blocks, which can be done in parallel for additional speed gains. Of note, blocking on an unreliable field can lead to missed matches, making this form of blocking undesirable in some situations \citep{steorts_comparison_2014}.

After computing all comparison vectors within a block, the modeller can further reduce the number of comparison vectors used in the linkage algorithm through indexing. For example, one might only consider pairs with a certain similarity score on a field deemed to be important, like first name, or only pairs that exactly match on a specified number of fields. However, the impact of indexing on model parameters is not well understood; \citep{murray2016probabilistic} reviewed this issue in the context of the frequentist Fellegi-Sunter model, leaving the effect of indexing on Bayesian record linkage models to future work.

With \texttt{fabl}, we introduce two techniques to further expand the scalability of probabilistic record linkage. First, we propose hashing methods that allow us to compute summary statistics that reduce the computational complexity of the Gibbs sampler. Second, we introduce storage efficient indexing, which reduces the memory costs associated with unlikely matches. For convenience, Table \ref{table_notation_B} summarizes the notation introduced throughout this section.

\begin{table}[t!]
	\centering
	\begin{tabular}[t!]{ll}
		Symbol & Description \\
		\hline
		$P$ & number of agreement patterns exhibited in $\gamma$ \\
		$h_p$ & agreement pattern $p$ \\
		$e(h_p)$ & one-hot encoding of agreement pattern $p$ \\
		$\gamma_{ij} = h_p$ & comparison vector between records $A_i$ and $B_j$ exhibits pattern $p$ \\
		$r_{p_j}$ & list of records in $A$ that share agreement pattern $p$ with record $B_j$ \\
		$N_{p_j}$ & number of records in $A$ that share agreement pattern $p$ with record $B_j$ \\
		$N_p^m$ & number of matching comparison vectors that exhibit pattern $p$ \\
		$N_p$ & number of total comparison vectors that exhibit pattern $p$ \\

		\hline
	\end{tabular}\caption{Summary of hashing notation.}\label{table_notation_B}
\end{table}

\hypertarget{data-representation-hashing-and-storage}{%
	\subsection{Data Representation, Hashing, and
		Storage}\label{data-representation-hashing-and-storage}}
	
	{\color{red}{NOTE: This section used to be organized 
		\begin{itemize}
			\item Hashing
			\item SEI
			\item Inference
		\end{itemize}
	but now it is organized
		\begin{itemize}
		\item Hashing
		\item Inference
		\item Scaling (chunking method)
		\item SEI
	\end{itemize}

I think inference is much more natural and after hashing. And having all the equations and updates written out in the inference part makes it clearer that SEI does not affect posterior updates at all. 
}}
	
Since each component $\gamma_{ij}^f$ is discrete, there are only finitely many possible realizations of the comparison vector $\gamma_{ij}$. Let $P$ be the number of patterns realized in $\gamma$ and note that it is bounded above by $P^{*} =  \prod_{f=1}^F (L_f + 1)$, where the additional count notes the possibility of missing values. This quantity is determined by $F$ and $L_f$, and does not scale with $n_A$ or $n_B$. To obtain a memory efficient representation of the data, we map the agreement pattern of each record pair to a unique integer. \cite{enamorado2019using} accomplished this through a hashing function, which we modify to explicitly handle missing values:
\begin{align}
	\label{eqn:hashing}
	h^{*}(\gamma_{ij}) = \sum_{f = 1}^F I_{obs}(\gamma_{ij}^f)2^{\gamma_{ij}^f + I(f>1)\sum_{d=1}^{f-1}(L_d)}.
\end{align}
We then map these integers to sequential integers $\{1, \ldots, P\}$. We refer to each unique agreement pattern as $h_p$, and to the set of unique agreement patterns as $\mathcal{P} = \{h_1, \ldots, h_P\}$. When the $(i,j)$ record pair exhibits agreement pattern $p$, we say $\gamma_{ij} = h_p$.

{\color{red}{In calculations, we will at times use the one-hot encoding of agreement pattern $h_p$, denoted $e(h_p)$. This is a vector of length $\sum_{f=1}^F L_f$ in which the $l + \sum_{k=1}^{f-1} L_k$ component is 1 when $\gamma_{ij}^f = l$, and 0 otherwise. A more complete example of the one-hot encoding is provided in Appendix \ref{app:ohe}.}} 

{\color{red}{
For example, with five fields with binary agreements, the number of possible patterns is bounded above by $P^{*} = 3^5 = 243$. Records $A_5$ and $B_7$ may exhibit agreement pattern $\gamma_{5,7} = (1, 1, 1, NA, 2)$, indicating exact agreement on the first three fields, missing information in the fourth field, and disagreement in the fifth field. Then (\ref{eqn:hashing}) gives $h^{*}(\gamma_{5,7}) = 2^1 + 2^3 + 2^5 + 0 + 2^{10} = 1066$. We map the unique values of $h^{*}(\gamma)$ to sequential integers (for in example in \texttt{R}, by converting the integers into factors, and then back to integers). If 1066 is mapped to, for example, 42, then we would have $\gamma_{5,7} = h_{42}.$ This agreement pattern has the one hot encoding $e(h_{42}) = (1, 0, 1, 0, 1, 0, 0, 0, 0, 1)$.
}}

We then identify the records in $A$ with comparison vectors corresponding to each pattern $p$ for each record $B_j$. We denote this set $r_{p_j} = \{i | \gamma_{ij} = h_p\}$, and collect all such sets in the nested list $\mathcal{R} = \{r_{p_j} | p \in \{1, \ldots, P\}, j \in \{1, \ldots, n_B\} \}$. We then compute the number of records in $A$ that share agreement pattern $p$ with record $B_j$, given by
\begin{align}\label{eqn:N}
N_{p_j} = |r_{p_j}| = \sum_{i=1}^{n_A} I(\gamma_{ij} = h_p).
\end{align}
We collect these counts in $\mathcal{N} = \{N_{p_j} |p \in 1, \ldots, P, j \in 1, \ldots, n_B \}$. 

{
\color{red}{
Together, the set $\tilde{\gamma} = \{\mathcal{P}, \mathcal{R}, \mathcal{N}\}$ fully characterizes the comparison matrix $\gamma$ with no loss of information. To see this, we employ the condensed notation
\begin{align}
	m_p =  p(\gamma_{ij} = h_p|Z_j = i) = \prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}
\end{align}
to express the probability that records $A_i$ and $B_j$ form agreement pattern $p$ given that they are a match. We similarly define $u_p$. Viewed through the perspective of agreement patterns, the likelihood in (\ref{eqn:likelihood}) is equivalent to
\begin{align}\label{likelihood_efficient}
	\mathcal{L}(\bm{Z}, \bm{m}, \bm{u} \mid \tilde{\gamma}) = \prod_{j=1}^{n_B}\prod_{p=1}^P \prod_{i \in r_{p_j}} m_p^{I(Z_j = i)}u_p^{1 - I(Z_j = i)}. 
\end{align}
}} 

\hypertarget{efficient-posterior}{%
	\subsection{Efficient Posterior Inference }\label{efficient-posterior}}

Using the likelihood in (\ref{likelihood_efficient}), we can see that the statistics collected in $\tilde{\gamma}$ allow for more efficient posterior inference for all model parameters. Conditioning on $\bm{Z}$, we can express the conditional likelihood for $\bm{m}$ and $\bm{u}$ as
\begin{align}
	\mathcal{L}(\bm{m}, \bm{u} \mid \gamma, \bm{Z})  &=  \prod_{p=1}^P m_p^{N_p^m}u_p^{N_p - N_p^m}  \label{likelihood_efficient_m_u}
\end{align}
where $N_p^m = \sum_{j=1}^{n_B} I\left(\gamma_{Z_j, j} = h_p \right)$ is the number of matching record pairs with agreement pattern $p$, and $N_p = \sum_{j=1}^{n_B} N_{p_j}$ is the total occurrence of pattern $p$ in the data across all record pairs. Note that the $\bm{m}$ and $\bm{u}$ parameters depend on the data only through the quantities calculated from $\mathcal{N}$ and $\mathcal{P}$. 

Additionally, let $\bm{\alpha_0} = (\alpha_{11}, \ldots, \alpha_{F L_F})$ be a concatenated vector of prior parameters for the $\bm{m}$ distributions, and define $\bm{\beta_0}$ similarly for the $\bm{u}$ distributions. {\color{red}{Using the one-hot encodings described in Section \ref{data-representation-hashing-and-storage},} the terms needed for the posterior updates for the $\bm{m}$ and $\bm{u}$ parameters are given by the appropriate components of the vectors
\begin{subequations}
	\begin{align}
		\bm{\alpha(Z)} &= \bm{\alpha_0} + \sum_{p=1}^P N_p^m \times e(h_p), \label{efficient_alpha} \\
		\bm{\beta(Z)} &= \bm{\beta_0} + \sum_{p=1}^P (N_p - N_p^m) \times e(h_p). \label{efficient_beta}
	\end{align}
\end{subequations}
Specifically, the $l + \sum_{k=1}^{f-1} L_k$ component of (\ref{efficient_alpha}) and (\ref{efficient_beta}) provides the the posterior updates for level $l$ and field $f$ in (\ref{eqn:alpha_update}),and (\ref{eqn:beta_update}). In this new form however, they are calculated through $P$ multiplications of scalar quantities over the vectors $h_p$, and then vectorized summations.} This is markedly more efficient than summing over $n_A n_B$ record pairs for each field and agreement level.

We can also use (\ref{likelihood_efficient}) to express the conditional likelihood for $Z_{j}$. Using $w_p = \frac{m_p}{u_p}$, we have
\begin{align}
	\mathcal{L}(Z_j\mid  \bm{m}, \bm{u}, \tilde{\gamma}) = \prod_{p=1}^P u_p^{N_{p_j}} \prod_{i \in r_{p_j}} w_p^{I(Z_j = i)}\label{likelihood_efficient_z}
\end{align}
Note that $Z_j$ interacts with the data only through $\mathcal{N}$ and $\mathcal{R}$. This likelihood also lends itself to markedly faster posterior computations. Although sampling $Z_j$ from the full conditional provided in (\ref{eqn:z_full_conditional}) is conceptually straightforward, it can become computationally expensive when $n_A$ is large. This is because sampling a value from $n_A$ options with unequal weights requires normalizing the weights to probabilities, which has a computational cost that scales with $n_A$. To speed up computation, we break this sampling step into two. We first sample among $P + 1$ options for the agreement pattern between $B_j$ and its potential link, according to
\begin{align}
	\label{eqn:gibbs1}
	P\left( \gamma_{Z_j^{(s+1)}, j} = h_p \mid \tilde{\gamma}, \bm{m}, \bm{u}, \bm{Z^{(s)}}\right) \propto
	\begin{cases} 
		N_{p_j} \times w_{p},  & \gamma_{Z_j^{(s+1)}, j} = h_p; \\
		n_A \frac{n_B - n_{AB}(\bm{Z}^{(s)}) + \beta_{\pi}}{n_{AB}(\bm{Z}^{(s)}) + \alpha_{\pi}}, &   \text{otherwise}. \\
	\end{cases}
\end{align}
Since all posterior updates are governed by the agreement patterns of the record pairs rather than the record labels themselves, we complete the entire Gibbs sampler first at the level of the $P$ agreement patterns. Since all records in $A$ sharing the same agreement pattern with $B_j$ are equally likely, we then sample among candidate records uniformally using
\begin{align}
	\label{eqn:gibbs2}
	P\left(Z_j^{(s+1)} = q \mid \gamma_{Z_j^{(s+1)}, j} = h_p \right) = \begin{cases} 
		\frac{1}{N_{p_j}}, & q \in r_{p_j}; \\
		0, & \text{otherwise.} \\
	\end{cases}
\end{align} 

These changes can greatly improve the speed of the sampler, and each can be parallelized if desired for additional computational speed-ups. We emphasize the computational gains of this split sampler through Lemma \ref{lemma:fabl}.

\begin{lemma}
	Recall that $n_A$ and $n_B$ are the number of records in $A$ and $B$, respectively. Let $F$ be the number of fields used for comparisons across records, and $P$ be the number of patterns that comparison vectors exhibit in $A \times B$. We assume $C$ cores available for parallelization and a Gibbs sampler with $T$ iterations. Then, the overall computational complexity of \texttt{fabl} is $O(\frac{F}{C} n_A n_B) + O(\frac{T}{C}n_B P)$.
	\label{lemma:fabl}
\end{lemma}
\begin{proof}
	We consider two steps: constructing the comparison vectors and the Gibbs sampler. The computational complexity of all pairwise comparisons across $A$ and $B$ is $O(F n_A n_B)$. The hashing procedure for all pairwise comparisons is also $O(F n_A n_B)$. With $B$ processors available, we can split these computations across $B$ equally sized partitions and compute these comparisons in parallel, so the complexity becomes $O(\frac{F}{B} n_A n_B)$. There are then trivial computational costs associated with synthesizing summary statistics across these partitions. 
	
	Without hashing, the computational complexity of updating the $\bm{m}$ and $\bm{u}$ parameters is $O(F n_A n_B)$. However, by doing calculations over the agreement patterns rather than the individual records, hashing reduces the overall complexity to $O(P)$. The complexity of updating $\bm{Z}$ sequentially at the record level is $O(n_A n_B)$. With hashing, we split this sampling into two steps. First, we sample the agreement pattern of the match with complexity $O(n_B P)$, and then we sample the record exhibiting that pattern with complexity $O(n_B)$. Thus, the complexity of sampling $\bm{Z}$ in a single iteration is $O(n_B P)$. Since $P << n_A$ in most applications, we have reduced the complexity of sampling $\bm{Z}$ from $O(F n_A n_B)$ under \texttt{BRL} to $O(n_B P)$ under \texttt{fabl}. With parallelization, this complexity is further reduced to $O(\frac{1}{C}n_B P)$, and so the entire Gibbs sampler has complexity $O(\frac{T}{C}n_B P)$
	In summary, the total computational complexity is $O(\frac{F}{C} n_A n_B) + O(\frac{T}{C}n_B P).$
\end{proof} 

\hypertarget{scaling}{%
	\subsection{Scaling to Large Linkage Tasks}\label{scaling}}

		For linkage tasks with large amounts of records, we can partition the two data files $A$ and $B$ into $t_A$ and $t_B$ smaller disjoint chunks $\{A^a | a = 1, \ldots, t_A \}$ and $\{B^b | b = 1, \ldots, t_B\}$ for more manageable computations. {\color{red}{For example, if $n_A = n_B = 50000$ and $t_A = t_B = 100$, each chunk $A^a$ and $B^b$ would contain 500 records.}} For each data file $A^a$, we conduct all-to-all comparisons with each $B^b$ to construct the comparison matrix $\gamma^{ab}$. We then conduct hashing, obtain the compressed $\tilde{\gamma}^{ab}$ for later calculations, and delete the larger $\gamma^{ab}$ from memory before continuing with the next chunk of data. In detail, we calculate
\begin{subequations}
	\begin{align}
		r_{p_j}^{ab} &= \{i | \gamma_{ij} = h_p, j \in B^b\}, \label{r_{p_j}^{ab}} \\
		N_{p_j}^{ab} &= |r_{p_j}^{ab}|, \label{N_{p_j}^{ab}}
	\end{align}
\end{subequations}
These can computed serially or in parallel. Summary statistics from each pairwise chunk comparison can be easily synthesized to recover summary statistics for the full comparison matrix $\gamma$. Specifically, we combine information and obtain the statistics comprising $\tilde{\gamma}$ through
\begin{subequations}
	\begin{align}
		r_{p_j} &= (r_{p_j}^{11}, \ldots, r_{p_j}^{ t_A t_B}) \text{ for } a = 1, \ldots, t_A \text{ and } b = 1, \ldots, t_B, \label{r_{p_j}} \\
		N_{p_j} &= \sum_{a = 1}^{t_A} \sum_{b = 1}^{t_B} N_{p_j}^{ab},\label{N_{p_j}}	\end{align}
\end{subequations} 

\hypertarget{SEI}{%
	\subsection{Storage Efficient Indexing}\label{SEI}}

As discussed in Section \ref{data-representation-hashing-and-storage}, storing the indices, patterns, and counts in $\tilde{\gamma}$ uses dramatically less memory than storing the full comparison matrix $\gamma$. However, the memory requirements of each are still quadratic in nature. For very large linkage tasks, recording the indices for all record pairs in $\mathcal{R}$ can become computationally burdensome. We next introduce storage efficient indexing (SEI), which allows us to compute $\mathcal{N}$ for all $n_A \times n_B$ record pairs, while greatly reducing the memory costs of $\mathcal{R}$ associated with unlikely matches. This allows all-to-all comparisons for substantially larger linkage tasks.

{\color{red}{All records $A_i$ that share agreement pattern $p$ with record $B_j$ have the same $w_{p}$. Therefore, these records have the same probability to be identified as the link for record $B_j$. Thus, we know that records $i \in r_{p_j}$ such that $N_{p_j}$ is large are unlikely to be sampled consistently enough to be deemed a match through the Bayes estimate. We know this regardless of the form of the agreement pattern itself, or its associated probabilities. Therefore, rather than store all of these record labels, we store only a small number $S$. For each $r_{p_j}^{ab}$, we sample $S$ indices without replacement to form $SEI(r_{p_j}^{ab})$. We collect these memory reduced lists to form $SEI(r_{p_j})$ as in (\ref{r_{p_j}}), and collect these to form $SEI(\mathcal{R})$.

Let $n_{A^a}$ and $n_{B^b}$ be the number of records in chunks $A^a$ and $B^b$ respectively. Instead of storing $n_A \times n_B$ record labels, with SEI we store at most $\sum_{a = 1}^{t_A}\sum_{b = b}^{t_A} n_{A^a} \times n_{B^b} \times P \times S$ labels. As shown in the full conditionals in (\ref{efficient_alpha}),  (\ref{efficient_beta}), (\ref{eqn:gibbs1}), and (\ref{eqn:gibbs2}), all original record pairs are still accounted for through $\mathcal{N}$, and thus we can proceed with posterior inference with the memory reduced $SEI(\tilde{\gamma}) = \{\mathcal{P}, SEI(\mathcal{R}), \mathcal{N}\}$. We provide guidance on choice of $S$ through a simulation in Section \ref{SEI-sensitivity}.

NOTE: I switched the order of the chunking and the SEI because SEI only makes sense to use when you split the comparisons into chunks. Its also clearer mathematically. I think it also avoids the issue of the overloaded notation that used to be in equations \ref{r_{p_j}}. The reviewer asked to keep the SEI in the notation to reflect how SEI changed the likelihood and the quantities of interest, but I think the sentence "As shown in the full conditionals in (\ref{efficient_alpha}),  (\ref{efficient_beta}), (\ref{eqn:gibbs1}), and (\ref{eqn:gibbs2}), all original record pairs are still accounted for through $\mathcal{N}$" makes things very clear. Thoughts?}
}


%{\color{red}{For example, if file $A$ with $n_A = 500$ contains record $A_1$ that is an exact match for record $B_5$ and 499 records with complete disagreement on all fields with record $B_5$, we would only store $S$ indices to represent the records with full disagreement.I added that last example. I don't love it though. Thoughts?}}



%{\color{red}{When the linkage task is large enough that all $n_A \times n_B$ comparison vectors cannot be stored in memory at the same time, we propose a modified approach where we construct comparison vectors and calculate $\mathcal{R}$ and $\mathcal{N}$ for smaller subsets of the data, and combine this information before conducting the Gibbs sampler. We use this approach in the case study in Section \ref{nltcs}, and provide details in Appendix \ref{app:chunks}}}.


	\section{Simulation Studies}
	\label{sec:simulations}
	
	We demonstrate the speed and accuracy of \texttt{fabl} as compared to \texttt{BRL} through several simulation studies. 
	
	\hypertarget{speed}{%
		\subsection{Speed}\label{speed}}
	
	In our first simulation, we generate comparison vectors from pre-specified distributions so that we can easily increase the size of the linkage problem. We use $F = 5$ binary comparisons with probabilities for matching and non-matching pairs shown in Table \ref{Tab:distributions}. For each record in $B$, we simulate $n_A$ comparison vectors, resulting in a comparison matrix $\gamma \in \mathbb{R}^{n_A n_B \times F}$. For $n_B/2$ of these records, there is no match in $A$, so we simulate $n_A$ comparison vectors from the $\bm{u}$ distribution. For the other $n_B/2$ of these records, there is one match in $A$, so we simulate $1$ comparison vector from the $\bm{m}$ distribution, and $n_A - 1$ comparison vectors from the $\bm{u}$ distribution. We compare the run-time of \texttt{fabl} against \texttt{BRL} as we increase $n_A$ and $n_B$. Since we have five binary comparison fields with no missingness, the number of unique patterns $P$ is bounded above by $2^5 = 32$, a bound which is consistently attained in simulations with more records.
	
%	We simulate these data for different values of $n_A$ and $n_B$, always with $n_B/2$ records in common across data files. and compare the run-time of \texttt{fabl} against \texttt{BRL}. Since we have 5 binary comparison fields, the number is unique patterns $P$ is bounded above by $2^5 = 32$, a bound which is consistently attained in simulations with more records. 
	
	 %\textcolor{blue}{DELETE? The $\bm{u}$ parameters are chosen to emulate what is seen in practice for first name, last name, day of birth, and month of birth. The parameters for year would vary largely by context, so we choose them here to adjust the desired difficulty of the linkage task.} 
	
	\begin{table}[t]
		\centering
		\begin{tabular}{lll|ll}
			\multicolumn{1}{c}{ } & \multicolumn{2}{c}{$\bm{m}$} & \multicolumn{2}{c}{$\bm{u}$} \\
			\cline{2-3} \cline{4-5}
			& Agree & Disagree & Agree & Disagree \\
			\hline
			First Name & $\frac{19}{20}$ & $\frac{1}{20}$ & $\frac{1}{100}$ &  $\frac{99}{100}$ \\ 
			Last Name & $\frac{19}{20}$ & $\frac{1}{20}$ & $\frac{1}{100}$ &  $\frac{99}{100}$ \\ 
			Day & $\frac{19}{20}$ & $\frac{1}{20}$ & $\frac{1}{30}$ &  $\frac{29}{30}$ \\ 
			Month & $\frac{19}{20}$ & $\frac{1}{20}$ & $\frac{1}{12}$ &  $\frac{11}{12}$ \\ 
			Year & $\frac{19}{20}$ & $\frac{1}{20}$ & $\frac{1}{12}$ &  $\frac{11}{12}$ \\  
			\hline
		\end{tabular}
		\caption{Probabilities used for $\bm{m}$ and $\bm{u}$ distributions in simulation study in Section \ref{speed}.}\label{Tab:distributions}
	\end{table}
	
	The Gibbs sampler in the implementation of \texttt{BRL} that we use is coded in C \citep{sadinle_bayesian_2017}. In contrast, we use non-optimized code written only in \texttt{R} for \texttt{fabl}.  While this complicates comparisons, and indeed disfavors \texttt{fabl}, the computational speed gains for \texttt{fabl} are still evident, especially for larger sample sizes.  Additionally, although \texttt{fabl} is amenable to parallelization, this simulation is run on a single core. Implementing \texttt{fabl} in C++ with parallelization for the hashing step and sampling the matching status of the record pairs should lead to even more computational gains.
	
	In Figure \ref{fig:speed1}, where we increase both $n_A$ and $n_B$, \texttt{BRL} is faster than \texttt{fabl} for low sample sizes, but \texttt{fabl} is significantly faster at handling larger sample sizes. In particular, run-time for \texttt{BRL} grows quadratically (or linearly with the size of both $A$ and $B$) while run-time for \texttt{fabl} grows linearly (in the size of only $B$).
	
	\begin{figure}[t]
		\begin{center} \includegraphics[width=0.6\textwidth]{../notes/figures/sadinle_speed_plot2} 
			\caption{Run-time for \texttt{BRL} and \texttt{fabl} to run 1000 Gibbs iterations, including the hashing step for \texttt{fabl}, for increasing values of both $n_A$ and $n_B$, as described in Section \ref{speed}. We see near quadratic growth in run-time for \texttt{BRL}, and near linear growth for \texttt{fabl}.}\label{fig:speed1}
		\end{center}
	\end{figure}
	
	In Figure \ref{fig:speed2}, where we fix $n_B = 500$, we see near linear growth for the run-time under \texttt{BRL} as $n_A$ increases, and much more static run-time under \texttt{fabl}. The slight increases in run-time for \texttt{fabl} are due primarily to the hashing step, which again can be run in parallel for large data. To illustrate that these trends are generalizeable to other specifications of the comparison vectors, we have included the run-time results for an additional simulation study, under different comparison vector settings, in Appendix \ref{app:appendix-speed}.
	
	\begin{figure}[h!]
		\begin{center} \includegraphics[width=0.6\textwidth]{../notes/figures/speed_plot_fixed_nB_slides} 
			\caption{Run-time for \texttt{BRL} and \texttt{fabl} to run 1000 Gibbs iterations, including hashing step for \texttt{fabl}, with $n_B$ fixed at 500, as described in Section \ref{speed}. We see near linear growth in run-time for \texttt{BRL}, and near constant run-time for \texttt{fabl}.}\label{fig:speed2}
		\end{center}
	\end{figure}
	
	
	\hypertarget{accuracy}{%
		\subsection{Accuracy}\label{accuracy}}
	
	Computational speed-ups are only worthwhile if not accompanied by a notable loss of record linkage accuracy. Therefore, we examine the accuracy of \texttt{fabl} relative to \texttt{BRL} by replicating a simulation study from \cite{sadinle_bayesian_2017}. The simulations employ a collection of synthetic data files with varying amounts of error and overlap (the number of records in common across files). Following methods proposed by \cite{christen_pudjijono2009} and \cite{christen_vatsalan2013}, clean records are first simulated from frequency tables for first name, last name, age, and occupation in Australia. Fields are then chosen for distortion uniformly at random. Names are subject to string insertions, deletions and substitutions, as well as common keyboard, phonetic, and optical recognition errors. Age and occupation are distorted through keyboard errors and missingness. These synthetic data files are available in the supplement to \cite{sadinle_bayesian_2017}.
	
	We create comparison vectors according to the default settings of the \texttt{compareRecords} function from the \texttt{BRL} package, shown in Table \ref{Tab:sadinle_simulation_cutoffs}. Each simulation identifies matched individuals between two data files, each with 500 records. We conduct linkage when matching records exhibit 1, 2, and 3 errors across the four fields, and when there are 50, 250, and 450 individuals in common across data files. Under each of these settings, we have 100 pairs of simulated data files in order to obtain uncertainty quantification on our performance metrics. We use uniform priors for all $\bm{m}$ and $\bm{u}$ parameters, with $\alpha_{fl} = \beta_{fl} = 1$ for all $f$ and $l$. We run the Gibbs sampler for 1000 iterations, and discard the first 100 as burn-in. We calculate Bayes estimates $\hat{\bm{Z}}$ of the linkage structure using the  loss function and post-processing procedure described in Appendix \ref{bayes-estimate}. Traceplots for parameters of interest for one example simulation are provided in Appendix \ref{app:appendix-sim}; they show no obvious concern over MCMC convergence. We also replicate this simulation allowing \texttt{fabl} to leave some components of the linkage structure undetermined and left for clerical review; these results are in Appendix \ref{partial}.
	
	\begin{table}[t]
		\centering
		\begin{tabular}[t]{llllll}
			
			\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Level of Disagreement} \\
			\cline{3-6}
			Fields & Similarity & 1 & 2 & 3 & 4\\
			\hline
			First and Last Name & Levenstein & 0 & (0, .25] & (.25, .5] & (.5, .1]\\
			Age and Occupation & Binary & Agree & Disagree &  & \\
			\hline
		\end{tabular}
		\caption{Construction of comparison vectors for accuracy study with simulated data files of Section \ref{accuracy}.}
		\label{Tab:sadinle_simulation_cutoffs}
	\end{table}
	
	We compare \texttt{fabl} to \texttt{BRL} in terms of recall, precision and F-measure, as defined in \cite{christen_2012}. Recall is the proportion of true matches found by the model, that is, $\sum_{j=1}^{n_B} I(\hat{Z}_j = Z_j, Z_j \leq n_A) / \sum_{j=1}^{n_B} I(Z_j \leq n_A)$. Precision is the proportion of links found by the model that are true matches, that is, $\sum_{j=1}^{n_B} I(\hat{Z}_j = Z_j, Z_j \leq n_A) / \sum_{j=1}^{n_B} I(\hat{Z}_j \leq n_A)$. The F-measure balances the two metrics to provide an overall measure of accuracy, and is defined as $2 \times (\text{Recall } + \text{ Precision}) / (\text{Recall } \times \text{ Precision})$. In Figure \ref{fig:sadinle_simulation}, we see that the two methods have comparable performance at all levels of error and overlap. In the specific case of high error and low overlap, widely regarded as the most difficult linkage scenario, we see that \texttt{fabl} performs slightly worse than \texttt{BRL} on average; however, the overall accuracy level remains high. 
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/sadinle_sim_plot2} 
			\caption{Posterior means and credible intervals for accuracy metrics under the replication of simulation study from \cite{sadinle_bayesian_2017}. For each level of overlap and each level of error, we have 100 paired sets of 500 records. Thus this table summarizes results for 900 data files. We see comparable performance for all levels of error and overlap.}
			\label{fig:sadinle_simulation}
		\end{center}
	\end{figure}

	\hypertarget{SEI-sensitivity}{%
	\subsection{SEI Sensitivity}\label{SEI-sensitivity}}

{
\color{red}{Finally, our last simulation demonstrates our method's robustness to different values of $S$ for the SEI memory reduction procedure. We perform record linkage on one set of synthetic datafile described in Section \ref{accuracy} with 500 records in each datafile, 250 entities in common across datafiles, and 3 errors present across matching records. To achieve more drastic results, we perform SEI without chunking the data, with $t_A = t_B = 1$. In practice, if it is possible to create and store the comparison matrix for all record pairs at one time, there is no need to reduce the memory of the hashed matrix through SEI. For illustration however, it is easier to see degenerate behavior in this setting. 

We preform linkage using SEI with $S = (1, 2, 5, 10, 20)$, and without using SEI, always with 500 iterations of the Gibbs sampler. 
Note that any particular SEI memory reduction may lead to better or worse linkage performance; if the SEI procedure happens to only remove pairs that are not match by random chance, recall and precision will improve. Therefore, we perform linkage under each setting 100 times, recording the linkage estimate $\hat{\bm{Z}}$, and recall and precision.

In Figure \ref{fig:SEI_Z},  see that the largest number of distinct linkage estimates occurs when $S = 1$. This makes sense, because the SEI procedure arbitrarily removes large numbers of record labels from consideration, resulting in a noisier estimate of the linkage structure. The number of distinct linkage estimates decreases as $S$ increases, with larger values of $S$ providing results more similar to the linkage without SEI. In Figure \ref{fig:SEI_eval}, we see similar patterns in precision. Setting $S=1$ can arbitrarily remove the index of a true match, leading the Gibbs sampler to concentrate probability on a false match, while larger values of $S$ produce results mirroring implementation with no SEI. We note however that even with $S=1$, the loss in precision is very small.

Although the figures provided suggest that $S=2$ is adequate for maintaining linkage performance, we suggest a more conservative value like $S=10$. When evaluating the performance of a record linkage algorithm, researchers often examine posterior probabilities. By concentrating probability mass on arbitrary nonmatches, low values of $S$ may induce suspiciously high posterior probability for certain record pairs, providing a warped perception of model performance. We want to be sure that the set of records pairs exhibiting agreement patterns that have considerable probability of matching do not get reduced through SEI.}
}

%\begin{figure}[t]
%	\begin{center}
%		\includegraphics[width=0.6\textwidth]{../SEI_sensitivity/Z_plot}
%		\caption{Distinct values of $\hat{Z}$ in Section \ref{SEI-sensitivity} simulation.}
%		\label{fig:SEI_Z}
%	\end{center}
%\end{figure}
%
%\begin{figure}[t]
%	\begin{center}
%		\includegraphics[width=0.6\textwidth]{../SEI_sensitivity/eval_plot} 
%		\caption{Means and 95\% credible intervals for recall and precision in Section \ref{SEI-sensitivity} simulation}
%		\label{fig:SEI_eval}
%	\end{center}
%\end{figure}
%
%  \begin{figure}[hbt]
%	\centering
%	\begin{subfigure}{0.4\textwidth}
%		\includegraphics[width=\textwidth]{../SEI_sensitivity/Z_plot} 
%		\caption{}\label{fig:SEI-Z}
%	\end{subfigure}\hspace{\columnsep}%
%	\begin{subfigure}{0.4\textwidth}
%		\includegraphics[width=\textwidth]{../SEI_sensitivity/eval_plot} 
%		\caption{}\label{fig:SEI-eval}
%	\end{subfigure}
%\caption{Simulation results Section \ref{SEI-sensitivity}. In \ref{fig:SEI-Z}, 
%\end{figure}
%Figure \ref{fig:here}

\begin{figure}
	\begin{minipage}[c]{0.4\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth]{../SEI_sensitivity/Z_plot}
		\caption{Distinct values of $\hat{Z}$ in Section \ref{SEI-sensitivity} simulation.}
		\label{fig:SEI_Z}
		\end{center}
	\end{minipage}
	\hfill
	\begin{minipage}[c]{0.5\linewidth}
		\begin{center}
		\includegraphics[width=\linewidth]{../SEI_sensitivity/eval_plot} 
		\caption{Means and 95\% credible intervals for recall and precision in Section \ref{SEI-sensitivity} simulation}
		\label{fig:SEI_eval}
	\end{center}
	\end{minipage}%
\end{figure}
	\section{Case Studies}
	\label{sec:case-studies}
	
	In our first case study, we revisit data from the El Salvadoran Civil War analyzed by \cite{sadinle_bayesian_2017}. Though the data files used in this case study are small, it shows how the computational complexity of \texttt{fabl} depends on the number of unique agreement patterns found in the data, and how significant computational gains can be achieved by simplifying the construction of the comparison vectors. In the second case study, we apply \texttt{fabl} to link records from the National Long Term Care Study, a larger linkage task that is not feasible in reasonable time under \texttt{BRL} with typical computing setups. 
	
	\subsection{Civilian Casualties from the El Salvadoran Civil War}
	\label{el_salvador}
	
	The country of El Salvador was immersed in civil war from 1980 to 1991, and we are interested in estimating the total number of casualties from the war. We utilize lists of casualties from the war, one collected by El Rescate - Tutela Regal (ERTL) and another from the Salvadoran Human Rights Commission (CDHES, by its acronym in Spanish).\footnote{We thank the Human Rights Data Analysis Group (HRDAG) for granting access to these data.} The ERTL dataset comprises digitized denunciations published throughout the conflict, and the CDHES dataset comprises casualties reported directly to the organization \citep{howland2008rescate, ball2000salvadoran}. The ERTL required additional investigation before recording denunciations as human rights abuses, and reports to the CHDES were made shortly after the events occurred; thus, both data files are thought to be fairly reliable. When estimating the total number of casualties, one cannot simply sum the numbers recorded by each organization, as it is likely that the same individuals are recorded in multiple casualty lists. Instead, record linkage techniques must be used to merge data files before analyzing the data \citep{lum2013applications}. 
	
	There are several challenges with these data. First, both data files have been automatically digitized, which inherently leads to some degree of typographical error. Second, the only fields recorded are given name, last name, date of death, and place of death. It is relatively common for a parent and child to share the same given name, resulting in indistinguishable records for two different individuals. 
	
	Following \cite{sadinle_bayesian_2017}, we utilize records that have non-missing entries for given and last name, which results in $n_A = 4420$ records in CHDES and $n_B = 1323$ records in ERTL. We standardize names to account for common misspellings and use a modified Levenstein distance when comparing names to account for the fact that second names are often omitted in Spanish. Place of death is recorded by municipality and department within that municipality; however, since department is missing in 95\% of records in CHDES and 80\% of records in ERTL, we exclude department from our analysis. Thus, we conduct record linkage using given name, last name, municipality, and day, month, and year of death. We use uniform priors for the $\bm{m}$ and $\bm{u}$ parameters.
	
	We initially followed the comparison vector constructions set by \cite{sadinle_bayesian_2017}, using four levels of agreement for each field, according to the thresholds provided in Table \ref{Tab:el_salvador_cutoffs_1}. This results in $5^5 \times 3 = 6025$ possible agreement patterns, with 1173 patterns realized in the data. However, we noticed that the posterior distributions of several levels of the $\bm{m}$ and $\bm{u}$ parameters were nearly identical in an initial run of \texttt{BRL}, suggesting that these levels were unnecessary.
	
	\begin{table}[t]
		\begin{tabular}[t]{llllll}
			\multicolumn{2}{c}{ } & \multicolumn{4}{c}{Level of Disagreement} \\
			\cline{3-6}
			Fields & Similarity & 1 & 2 & 3 & 4\\
			\hline
			First and Last Name & Modified Levenstein & 0 & (0, .25] & (.25, .5] & (.5, 1]\\
			Year of Death & Absolute Difference & 0 & 1 & 2 & 3+\\
			Month of Death & Absolute Difference & 0 & 1 & 2-3 & 4+\\
			Day of Death & Absolute Difference & 0 & 1-2 & 3-7 & 8+\\
			Municipality & Binary & Agree & Disagree &  & \\
			\hline
		\end{tabular}
		\caption{Construction of comparison vectors for El Salvador data resembling original implementation from \cite{sadinle_bayesian_2017}. This setup leads to 2048 possible agreement patterns in total.}\label{Tab:el_salvador_cutoffs_1}
	\end{table}

	\begin{table}[t]
	\centering
	\begin{tabular}[t]{lllll}
		\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Level of Disagreement} \\
		\cline{3-5}
		Fields & Similarity & 1 & 2 & 3\\
		\hline
		First and Last Name & Modified Levenstein & 0 & (0, .25] & (.25, 1]\\
		Year of Death & Binary & Agree & Disagree & \\
		Month of Death & Binary & Agree & Disagree & \\
		Day of Death & Absolute Difference & 0 & 1 & 2+\\
		Municipality & Binary & Agree & Disagree & \\
		\hline
	\end{tabular}
	\caption{Construction of comparison vectors for El Salvador data for increased speed under \texttt{fabl}. This setup leads to 216 possible agreement patterns in total.}\label{Tab:el_salvador_cutoffs_2}
\end{table}
	
	Therefore, we perform our analysis with the agreement levels for each field according to Table \ref{Tab:el_salvador_cutoffs_2}. Among the 216 possible agreement patterns, 159 are realized in the data. With this revised comparison specification, \texttt{fabl} runs in 61 seconds, approximately 4 times faster than the \texttt{BRL} run time of 239 seconds.  The estimates of the $\bm{m}$ parameters under each method are similar, as shown in Figure \ref{fig:m-and-u}. Estimates of $\bm{u}$ are indistinguishable, and thus omitted. Traceplots for parameters of interest are provided in Appendix \ref{app:appendix-es}.
	
	For completeness, we note that linkage with the more detailed comparison vectors requires 240 seconds for \texttt{BRL}, and 261 seconds for \texttt{fabl}.  Apparently, the number of patterns is sufficiently many that the computational savings from \texttt{fabl} does not overcome the inherent speed differences of C as opposed to \texttt{R}.
	
	Through \texttt{fabl}, we arrive at a Bayes estimate of 179 individuals recorded in both data files. We calculate posterior samples of the size of the overlap across files by finding the number of links in each iteration of the Gibbs sampler, and subtracting the number of matches that violate one-to-one matching. The posterior 95\% credible interval for the overlap across files is (206, 238), indicating that the Bayes estimate identifies fewer matches than the Gibbs sampler identifies on average. This is because a large number of records in ERTL have multiple plausible matches in CDHES; \texttt{fabl} recognizes that a match exists among the several options, but is unable to definitely declare a specific pair as a match in the Bayes estimate. We see similar results under \texttt{BRL}, with a Bayes estimate of 181 individuals recorded in both data files, and a posterior 95\% credible interval of (211, 244). See Figure \ref{fig:overlap-plot} for a visual comparison of the Bayes estimates and posterior credible intervals for the two methods. We note that Bayes estimates falling outside of posterior credible intervals has been observed previously in the record linkage literature \citep{sadinle_bayesian_2017, steorts_bayesian_2016}, and remains a topic for future research.
	
	%We also compute a partial estimate of the linkage structure, using $\theta_{10} = \theta_{01} = 1$, $\theta_{11} = 2$, and $\theta_R = 0.1$ as in the simulation study in Appendix \ref{partial}. Here, the Bayes estimate provides 136 matches of which the model is quite confident, and 175 records to verify manually. This means that after clerical review, the number of individuals replicated across data files would fall in the interval (136, 311), encapsulating the posterior credible interval. More or fewer records could be identified for clerical review by decreasing or increasing $\theta_R$. 
	
	%and a range of (140, 294) after the partial estimate and clerical review. 
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/overlap_distribution_smallP_bayes}
			\caption{Posterior distribution and Bayes estimate of overlap across the two files. We note they are quite similar under both methods.}
			\label{fig:overlap-plot}
		\end{center}
	\end{figure}
	
	
	\begin{figure}[t]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/m_posterior_smallP} 
			\caption{Posterior estimates of $\bm{m}$ parameters with 95\% credible intervals for the El Salvador case study. They are quite similar across the two methods.}\label{fig:m-and-u}
			\label{fig:m-and-u}
		\end{center}
	\end{figure}
	
	\subsection{National Long Term Care Study}
	\label{nltcs}
	
	\begin{table}[t]
		\centering
		\begin{tabular}[t]{lllll}
			\multicolumn{2}{c}{ } & \multicolumn{3}{c}{Level of Disagreement} \\
			\cline{3-5}
			Fields & Similarity & 1 & 2 & 3\\
			\hline
			Sex & Binary & Agree & Disagree & \\
			Year of Birth & Binary & Agree & Disagree & \\
			Month of Birth & Binary & Agree & Disagree & \\
			Day of Birth & Binary & Agree & Disagree & \\
			Location & Custom & Same State and Office & Same State & Otherwise \\
			\hline
		\end{tabular}
		\caption{Construction of comparison vectors for NTLCS data.}\label{Tab:nltcs-comparisons}
	\end{table}
	
	The National Long Term Care Study (NLTCS) is a longitudinal study tracking the health outcomes of Medicare recipients \citep{steorts_bayesian_2016}. The initial survey began in 1982, with follow-up surveys taken approximately every five years. As such, patients are surveyed at most once in a given year, and many patients are surveyed across multiple years. In addition, patients can either drop out of the study, pass away, or enter as new patients. Hence, the assumptions of our model hold for this study. We seek to link records over the $n_A = 20485$ individuals from 1982 to the $n_B = 17466$ individuals from 1989. The NLTCS data have longitudinal links, so that in reality one does not need to conduct record linkage. However, following the strategy in \cite{guha:reiter:BA}, we break the longitudinal links and treat the data from 1982 and 1989 as stand-alone data files.
	
	We link records using sex, date of birth, and location using the thresholds shown in Table \ref{Tab:nltcs-comparisons}. Storing three comparison scores for each of $20485 \times 17466 \approx 400,000,000$ record pairs would require approximately 8GB of memory. Standard settings on a 16GB personal computer do not allow storage of an object of this size, and thus \texttt{BRL} is unable to perform this linkage task on such a machine. However, through the \texttt{fabl} framework, we compute comparisons over 30 smaller comparison tasks, hash results, and combine results before conducting linkage. Without SEI, the resulting data object is about 2.2 GB, and with SEI using $S=10$, the resulting data object is about 760 MB. Constructing the comparisons sequentially took approximately 40 minutes, which could be reduced considerably through parallel computing. 
	
	We run a Gibbs sampler for 1000 iterations, taking about 235 seconds. As shown in Figure \ref{fig:nltcs-overlap-plot}, the Bayes estimate of the linkage structure consists of 9634 matches, with a 95\% credible interval of (9581, 9740). Since we have access to the true linkage structure, we can calculate recall to be 0.89 and precision to be 0.98, resulting in an F-measure of 0.94. Traceplots do not suggest convergence issues, and are similar to those seen in Appendix \ref{app:appendix-sim} and \ref{app:appendix-es}
	
	\begin{figure}[h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/nltcs/overlap_posterior4}
			\caption{Posterior distribution and Bayes estimate of overlap across years 1982 and 1989 of NLTCS data.}
			\label{fig:nltcs-overlap-plot}
		\end{center}
	\end{figure}
	
	%\hypertarget{violations}{%
		%	\subsection{Violations of One-to-One Matching}\label{violations}}
	%
	%As noted previously, the application exhibits many situations in which bipartite matching is difficult. To explore further, we calculate the number of matchings throughout the \texttt{fabl} Gibbs sampler that violate our one-to-one assumption, and find on average 26.17 matchings in violation. These matchings are impossible under the \texttt{BRL} Gibbs sampler, so examining these more closely can provide insight how each model handles situations in which one file in $A$ has
	%multiple plausible matches in $B$. 
	%
	%Table \ref{Tab:rosa-maria} illustrates an example. Note that these records present a near violation of our assumption that there are no duplications within files; we continue to assume that records 825 and 826 in ERTL correspond to different individuals (most likely a mother and daughter), but their records are nearly identical. In addition, using the modified Levenstein distance, the comparison vectors $\gamma_{2776, 825}$ and $\gamma_{2776, 826}$ are exactly identical.
	%
	%\begin{table}
	%	\centering
	%	\begin{tabular}[h!]{llllllll}
		%		\hline
		%		Record No. & Dataset & Last Name & First Name & Day & Month & Year & Dept \\
		%		\hline
		%		825 & CDHES & Pineda & Rosa & 6 & 4 & 1984 & NA\\
		%		826 & CDHES & Pineda & Rosa Maria & 6 & 4 & 1984 & NA\\
		%		2776 & ERTL & Pineda & Rosa Maria & 4 & 4 & 1984 & Cuscatlan\\
		%		\hline
		%	\end{tabular}
	%	\caption{Example of linkage situation in which record 2776 in ERTL has multiple plausible matches in CDHES, leading to undesirable behavior both \texttt{fabl} and \texttt{BRL}.}\label{Tab:rosa-maria}
	%\end{table}
	%
	%Figure~\ref{fig:mixing-plot} shows the values that $Z_{825}$ and $Z_{826}$ take on throughout the Gibbs sampler, and demonstrates how each method handles this situation. Under \texttt{fabl}, both records in $B$ match to the same record in $A$ throughout the Gibbs process, creating consistent violations of one-to-one matching. Under \texttt{BRL}, the Gibbs process creates one matching configuration and stays there for a while. However, if one pair ``unmatches,'' then the other record has a chance to latch on. Then, the Gibbs process is stuck with that matching status for a while, resulting in a Gibbs process with poor mixing. 
	%
	%Through its independent sampling,\texttt{fabl} allows the modeler to inspect records with multiple plausible matches, and if they desire, to then choose the record pairing with the highest posterior probability. \texttt{BRL} in contrast, in strictly enforcing one-to-one matching throughout the sampler, can lead to situations where none of the plausible matches reach the threshold to be identified through the Bayes estimate. Thus relaxing the one-to-one constraint has served as a useful tool for identifying model misspecification. 
	%
	%\begin{figure}[h!]
	%\begin{center}
	%\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/bad_mixing} 
	%\caption{Gibbs sampling in situation with multiple plausible matches. Under \texttt{BRL}, two records in ERTL compete for their most similar record, splitting the posterior probability. Under \texttt{fabl}, both record consistently match with their most similar record, consistently violating one-to-one matching.} \label{fig:mixing-plot}
	%\end{center}
	%\end{figure}
	
	\section{Conclusion}
	\label{discussion}
	
	In this paper, we have proposed \texttt{fabl}, a Bayesian record linkage method that extends the work of \cite{sadinle_bayesian_2017} to scale to large data sets. We have proven that the proposed hashing method and model assumptions allow for a linkage procedure whose computational complexity does not scale with the size of the larger data file. This makes \texttt{fabl} computationally advantageous in many linkage scenarios, particularly when one data file is substantially smaller than the other. We have also shown that storage efficient indexing, in tandem with hashing, greatly reduces the memory costs required for all-to-all comparisons, giving practitioners an option for larger record linkage tasks potentially even without the use of blocking or indexing. We have demonstrated the speed and accuracy of \texttt{fabl} by replicating a simulation study and a case study in \cite{sadinle_bayesian_2017}, and through an additional case study that is computationally impractical under \texttt{BRL}. 
	
	Although the \texttt{fabl} method greatly reduces the memory costs for all-to-all comparisons, computing all $n_A \times n_B$ record pairs still can be prohibitive for larger linkage tasks. Indeed, constructing the comparison vectors for the NLTCS linkage task involving around 40,000 records in Section \ref{nltcs} took around 40 minutes. Due to the quadratic nature of the comparison space, this computation time would grow quickly with the size of the linkage task, and would be infeasibly slow when dealing with millions of records. Although it is common to use deterministic blocking to reduce the comparison space and then apply probabilistic record linkage within each block, issues arise when sizes of blocks vary across the linkage task. In future work, we seek to extend \texttt{fabl} to account for such deterministic blocking, making the framework amenable to arbitrarily large linkage tasks.

	
	\clearpage
	
	\bigskip
	
	\bibliographystyle{jasa}
	\bibliography{biblio}
	
	\clearpage
	
	\section{Appendix}
	\label{sec:appendix}
	
	\hypertarget{bayes-estimate}{%
		\subsection{Bayes Estimate}
		\label{bayes-estimate}}
	
We calculate a Bayes estimate $\hat{\bm{Z}}$ for the linkage parameter $\bm{Z}$ by assigning different positive losses to different types of errors, and minimizing posterior expected loss. We adopt the loss function proposed in \cite{sadinle_bayesian_2017} in which $\hat{Z}_j \in \{1, \ldots, n_A, n_A + j, R\}$, with $R$ representing the option to leave the matching undetermined by the model. Specifically, we have
	\[L(\hat{Z_j}, Z_j)=\begin{cases} 
		0,  & \text{if } Z_j = \hat{Z_j}; \\
		\theta_R,  & \text{if } \hat{Z_j} = R; \\
		\theta_{10},  & \text{if } Z_j \leq 1,\hat{Z_j} = n_A + j ; \\
		\theta_{01},  & \text{if } Z_j = n_A + j,\hat{Z_j} \leq n_A ; \\
		\theta_{11},  & \text{if } Z_j \leq n_A, \hat{Z}_j \leq n_A, Z_j \neq \hat{Z_j}. \\
	\end{cases}\] 
Here, $\theta_R$ is the loss from not making a decision on the linkage status, $\theta_{10}$ is the loss from a false non-match, $\theta_{01}$ is the loss from a false match, and $\theta_{11}$ is the loss from the special case of a false match in which the record has a true match other than the one estimated by the model. 

In general, we set $(\theta_{10}, \theta_{01}, \theta_{11}, \theta_R) = (1, 1, 2, \infty)$ inducing the decision rule
	$$\hat{Z}_j =\begin{cases} 
		i,  & \text{if } P(Z_j = i |\gamma) > \frac{1}{2}; \\
		0,  & \text{otherwise}. \\
	\end{cases}$$
Since \texttt{fabl} does not strictly enforce one-to-one matching, it is possible for this Bayes estimate to link multiple records in $\bm{X_2}$ to one record in $\bm{X_1}$. In the event that we have two records $B_j$ and $B_{j'}$ such that both $P(\hat{Z}_j = i |\gamma) > \frac{1}{2}$ and $ P(\hat{Z}_{j'} = i |\gamma) > \frac{1}{2}$, we accept the match with the higher posterior probability, and declare the other to have no match. Since each $Z_j$ is independent, this is equivalent to minimizing the expected loss subject to the constraint that $\hat{Z}_j \neq \hat{Z}_{j'}$ for all $j \neq j'$.  A similar approach appears in the most probable maximal matching sets used by \cite{steorts_bayesian_2016} to match records to latent entities.

When we seek a partial estimate of the linkage structure, leaving a portion of record pairs to be classified manually in clerical review, we adopt losses $(\theta_{10}, \theta_{01}, \theta_{11}, \theta_R) = (1, 1, 2, .1)$. For a more in-depth explanation of this function and the induced Bayes estimate, see \cite{sadinle_bayesian_2017}.
	
%	\hypertarget{app:glossary}{%
%	\subsection{Summary of Notation}\label{app:glossary}}
%	
%		\begin{table}[h!]
%		\centering
%		\begin{tabular}[h!]{ll}
%			\hline
%			Symbol & Description \\
%			\hline
%			$A, B$ & data files\\
%			$i \in 1, \ldots, n_A $ & index over records in $A$\\
%			$j \in 1, \ldots, n_B $ & index over records in $B$\\
%			$f \in 1, \ldots F$ & index over fields used for comparisons \\
%			$l \in 1, \ldots L_f$ & index over agreement levels for field $f$ \\
%			$n_{AB}$ & number of entities in common between $A$ and $B$\\
%			$\gamma_{ij}$ & comparison vector for records $A_i$ and $B_j$ \\
%			$Z_j = i$ & records $A_i$ and $B_j$ match \\
%			$Z_j = n_A + j$ & record $B_j$ has no match in $A$ \\
%			$m_{fl}$ & $P(\gamma_{ij}^f = l | Z_j = i)$ \\
%			$u_{fl}$ & $P(\gamma_{ij}^f = l | Z_j \neq i)$ \\
%			$\pi$ & probability that a record in $B$ has a match in $A$ \\
%			$r_{p_j}$ & list of records in $A$ that share agreement pattern $p$ with record $B_j$ \\
%			$h_p$ & one hot encoding of agreement pattern $p$ \\
%			$H_{p_j}$ & number of records in $A$ that share agreement pattern $p$ with record $B_j$ \\
%			$H_p$ & number of total comparison vectors that exhibit pattern $p$ \\
%			$H_p^m$ & number of matching comparison vectors that exhibit pattern $p$ \\
%			$H_p^u$ & number of non-matching comparison vectors that exhibit pattern $p$ \\
%			\hline
%		\end{tabular}
%	\end{table}
	
	\hypertarget{app:derivations}{%
	\subsection{Derivations of Full Conditionals}\label{app:derivations}}

We provide detailed derivations of the full-conditionals provided in Section \ref{gibbs_sampling}. The $\bm{m}$ and $\bm{u}$ parameters are updated through standard multinomial-Dirichlet distributions. For a particular $m_{fl}$, we have
\begin{align*}
	p(m_{fl}| \gamma, \bm{Z}) &\propto \prod_{i=1}^{n_A} \prod_{j=1}^{n_B} m_{fl}^{I(Z_j = i) I(\gamma_{ij}^f = l) I_{obs}(\gamma_{ij}^f)} \times  m_{fl}^{\alpha_{fl} - 1} \\
	&= m_{fl}^{\alpha_{fl}(\bm{Z}) - 1},
\end{align*}
where $\alpha_{fl}(\bm{Z})= \alpha_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f)I(\gamma_{ij}^f = l) I(Z_j = i)$. Analogous procedures lead to the posterior distribution $p(u_{fl}| \gamma, \bm{Z})$  $\propto u_{fl}^{\beta_{fl}(\bm{Z}) - 1}$, where $\beta_{fl}(\bm{Z})= \beta_{fl} + \sum_{i,j} I_{obs}(\gamma_{ij}^f)I(\gamma_{ij}^f = l) I(Z_j \neq i)$. Thus for the vectors of parameters $\bm{m}_f$ and $\bm{u}_f$, we have

$$\bm{m}_f|\bm{Z}, \gamma \sim \text{Dirichlet}(\alpha_{f1}(\bm{Z}), \ldots, \alpha_{fL_f}(\bm{Z})),$$
$$\bm{u}_f|\bm{Z}, \gamma \sim \text{Dirichlet}(\beta_{f1}(\bm{Z}), \ldots, \beta_{fL_f}(\bm{Z})).$$

Since $\pi$ encodes the rate of matching across the two data files, the posterior distribution $p(\pi|\gamma, \bm{Z}, \bm{m}, \bm{u}, \alpha_{\pi}, \beta_{\pi})$ depends only on the number of links $n_{AB}(\bm{Z}) = \sum_{i=1}^{n_B}I(Z_j < n_A + j)$ encoded by $\bm{Z}$ (and hyperparameters). Thus, we use $p(\pi | \bm{Z}, \alpha_{\pi}, \beta_{\pi})$ and have 
\begin{align*}
	p(\pi | \bm{Z}, \alpha_{\pi}, \beta_{\pi}) &\propto p(\bm{Z}|\pi)p(\pi) \\
	&\propto \pi^{n_{AB}(\bm{Z})} (1-\pi)^{n_B - n_{AB}(\bm{Z})} \pi^{\alpha_{\pi} -1} (1-\pi)^{\beta_{\pi} -1} \\
	&\propto \pi^{n_{AB}(\bm{Z}) + \alpha_{\pi} - 1} (1-\pi)^{n_A - n_{AB}(\bm{Z}) + \beta_{\pi} -1}.
\end{align*}
Thus $\pi^{(s+1)}|\bm{Z}^{(s+1)},  \alpha_{\pi}, \beta_{\pi}$ has a $\text{Beta}(n_{AB}(\bm{Z}) + \alpha_{\pi}, n_B - n_{AB}(\bm{Z}) + \beta_{\pi})$ distribution.

To express the full conditional for $\bm{Z}$, we consider the likelihood in two cases. Because we sample each $Z_j$ independently of all other $Z_{j'}$, we use only the full conditional for an individual $Z_j$. Following the observation of \cite{wortman2019}, when $B_j$ does not link to any record in $A$, the contribution to the likelihood is simply a product of $u$ parameters, which we will call $c_j$:
\begin{align}
	p(\Gamma_{.j}| \bm{m}, \bm{u}, Z_j = n_A + j) = \prod_{i=1}^{n_A}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} = c_j.
\end{align}
When $Z_j = q$ for some $q\leq n_A$, we have
\begin{align}
	p(\Gamma_{.j}| \bm{m}, \bm{u}, Z_j = q) =\prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(\gamma_{qj}^f = l)I_{obs}(\gamma_{qj}^f)} \times \prod_{i \neq q}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}.
\end{align}
We multiply and divide by the $u$ parameters for the matching record pair to obtain
\begin{align}
	p(\Gamma_{.j}| \bm{m}, \bm{u}, Z_j = q) &\propto \prod_{f=1}^{F}\prod_{l=1}^{L_f} \left(\frac{m_{fl}}{u_{fl}}\right)^{I(\gamma_{qj}^f = l)I_{obs}(\gamma_{qj}^f)} \times \prod_{i = 1}^{n_A}\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} \\
	&= w_{qj} \times c_j.
\end{align}
We can divide the result of each case by $c_j$ to get
\begin{align}
	p(\Gamma_{.j}| \bm{m}, \bm{u}, Z_j) = \begin{cases} 
		w_{qj}, & q \leq n_A; \\
		1, &  q  = n_A + j. \
	\end{cases}
\end{align}
Lastly, we multiply the likelihood by the fast beta prior in (\ref{eqn:fast_beta_prior}) to obtain
\begin{align}
	\label{eqn:z_full_conditional2}
	p\left(Z_j^{(s+1)}  = q|\gamma, \bm{m}^{(s+1)}, \bm{u}^{(s+1)}, \bm{Z^{(s)}}\right) \propto
	\begin{cases} 
		\frac{\pi^{(s+1)}}{n_A} w_{q, j}^{(s+1)},  & q \leq n_A; \\
		1 - \pi^{(s+1)}, & q  = n_A + j. \
	\end{cases}
\end{align}

%Let $\gamma_{.j}$ denote the set of $n_A$ comparison vectors with $B_j$. We have
%\begin{align*}
%	p(\gamma_{.j}|Z_j = q, \bm{m}, \bm{u}) &\propto \prod_{i=1}^{n_A}\left[\prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(Z_j = i)}u_{fl}^{I(Z_j \neq i)}\right]^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}\\
%	&\propto \prod_{i=1}^{n_A}\frac{\left[\prod_{f=1}^{F}\prod_{l=1}^{L_f} m_{fl}^{I(Z_j = i)}u_{fl}^{I(Z_j \neq i)}\right]^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}}{\prod_{f=1}^{F}\prod_{l=1}^{L_f} u_{fl}^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}}\\
%	&\propto \prod_{i=1}^{n_A}\left(\prod_{f=1}^{F}\prod_{l=1}^{L_f} \frac{m_{fl}}{u_{fl}}\right)^{I(q = i) I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)} \\
%	&=
%	\begin{cases} 
%		w_{q, j}  & q \leq n_A; \\
%		1 &  q  = n_A + j. \
%	\end{cases}\\
%\end{align*}
%where
%$$w_{ij} = \prod_{f=1}^{F}\prod_{l = 1}^{L_f} \left(\frac{m_{fl}}{u_{fl}}\right)^{I(\gamma_{ij}^f = l)I_{obs}(\gamma_{ij}^f)}.$$

%With the likelihood in this form, we derive an expression for the posterior distribution of $\bm{Z}$. Because we sample each $Z_j$ independently of all other $Z_{j'}$, we use only the full conditional for an individual $Z_j$:
%\newpage
%\begin{align*}
%	p&\left(Z_j^{(s+1)} = q|\gamma_{.j}, \bm{m}, \bm{u}, \bm{Z^{(s)}}, \pi\right) \\
%	&\propto p(\gamma_{.j}| Z_j^{(s+1)}, \bm{m}, \bm{u}) P(Z_j^{(s+1)}|\pi) \\
%	&\propto \left(\sum_{i=1}^{n_A}w_{q, j}I(q = i) + I(q = n_A + j) \right) \left (\pi\sum_{i=1}^{n_A}\frac{1}{n_A}I(q = i) + (1-\pi)I(q = n_A + j) \right) \\	
%	&= \frac{\pi}{n_A}\sum_{i=1}^{n_A}w_{q, j}I(q = i) + (1-\pi)I(q = n_A + j). \\
%	&= \begin{cases} 
%		\frac{\pi}{n_A}w_{q, j}   & q \leq n_A; \\
%		1-\pi &  q  = n_A + j. \\
%	\end{cases}
%\end{align*}
%
%For more direct comparability with the method from \cite{sadinle_bayesian_2017}, we avoid sampling $\pi$ directly, and instead integrate over it in the final full conditional for $Z_j$.
%\begin{align*}
%	p&\left(Z_j^{(s+1)}  = q| \gamma, \bm{m}, \bm{u}, \bm{Z^{(s)}}\right)\\
%	&= \int_{\pi}p\left(Z_j^{(s+1)} | \gamma, \bm{m}, \bm{u}, \bm{Z^{(s)}}, \pi\right) p\left(\pi| \bm{Z^{(s)}}\right) d\pi \\
%	&= \int_{\pi}\left[\frac{\pi}{n_A}w_{z_{j}, j} I(q\leq n_A) + (1-\pi)I(q = n_A + j)\right] p\left(\pi| \bm{Z^{(s)}}\right) d\pi \\
%	&= \frac{\int_{\pi} \pi p\left(\pi| \bm{Z^{(s)}}\right) d\pi}{n_A}w_{z_{j}, j} I(q\leq n_A) + \left(1-\int_{\pi} \pi p\left(\pi| \bm{Z^{(s)}}\right) d\pi \right) I(q = n_A + j) \\
%	&= \frac{n_{AB}(\bm{Z}) + \alpha_{\pi}}{n_A (n_B + \alpha_{\pi} + \beta_{\pi})}w_{z_{j}, j} I(q\leq n_A) + \frac{n_B - n_{AB}(\bm{Z}) + \beta_{\pi}}{n_B + \alpha_{\pi} + \beta_{\pi}}I(q = n_A + j) \\
%	&\propto w_{q, j}I(q\leq n_A) + n_A \frac{n_B - n_{AB}(\bm{Z}) + \beta_{\pi}}{n_{AB}(\bm{Z}) + \alpha_{\pi}}I(q = n_A + j) \\
%	&=
%	\begin{cases} 
%		w_{q, j}  & q \leq n_A; \\
%		n_A \frac{n_B - n_{AB}(\bm{Z}) + \beta_{\pi}}{n_{AB}(\bm{Z}) + \alpha_{\pi}} & q   = n_A + j. \\
%	\end{cases} \\
%\end{align*}

	\clearpage
	
%	\hypertarget{app:chunks}{%
%	\subsection{SEI for Large Linkage Tasks}\label{app:chunks}}
	
%	For linkage tasks with large amounts of records, we can partition the two data files $A$ and $B$ into $t_A$ and $t_B$ smaller disjoint chunks $\{A^a | a = 1, \ldots, t_A \}$ and $\{B^b | b = 1, \ldots, t_B\}$ for more manageable computations. {\color{red}{For example, if $n_A = n_B = 500$ and $t_A = t_B = 10$, each chunk $A^a$ and $B^b$ would contain 50 records. (Jerry asked for more precision here. Does this example work?) }}For each data file $A^a$, we conduct all-to-all comparisons with each $B^b$ to construct the comparison matrix $\gamma^{ab}$. We then conduct hashing, obtain the compressed $\tilde{\gamma}^{ab}$ for later calculations, and delete the larger $\gamma^{ab}$ from memory before continuing with the next chunk of data. In detail, we calculate
%	\begin{subequations}
%		\begin{align}
%			r_{p_j}^{ab} &= \{i \in  | \gamma_{ij} = h_p, j \in B^b\}, \label{r_{p_j}^{ab}} \\
%			N_{p_j}^{ab} &= |r_{p_j}^{ab}|, \label{N_{p_j}^{ab}}
%		\end{align}
%	\end{subequations}
%	These can computed serially or in parallel. We reduce memory costs through SEI on each $r_{p_j}$, while retaining counts of each agreement pattern through $N_{p_j}^{ab}$. Summary statistics from each pairwise chunk comparison can be easily synthesized to recover summary statistics for the full comparison matrix $\gamma$. Specifically, we combine information through
%	\begin{subequations}
%		\begin{align}
%			r_{p_j}^{SEI} &= (r_{p_j}^{SEI, 11}, \ldots, r_{p_j}^{SEI, A^{*}B^{*}}) \text{ for } a = 1, \ldots, A^{*} \text{ and } b = 1, \ldots, B^{*},\label{r_{p_j}} \\
%			N_{p_j} &= \sum_{a = 1}^{A^{*}} \sum_{b = 1}^{B^{*}} N_{p_j}^{ab},\label{N_{p_j}}	\end{align}
%	\end{subequations}
%	
%	{\color{red}{Note that with this approach, each $r_{p_j}^{SEI}$ can contain up to $S \times t_A \times t_B$ indices, so we store at most  $n_B \times P \times S \times t_A \times t_B$ indices in total.}}
	
	
%		For linkage tasks with large amounts of records, we can partition the two data files $A$ and $B$ into $t_A$ and $t_B$ smaller disjoint chunks $\{A^a | a = 1, \ldots, t_A \}$ and $\{B^b | b = 1, \ldots, t_B\}$ for more manageable computations. {\color{red}{For example, if $n_A = n_B = 50000$ and $t_A = t_B = 100$, each chunk $A^a$ and $B^b$ would contain 500 records. (Jerry asked for more precision here. Does this example work?) }}For each data file $A^a$, we conduct all-to-all comparisons with each $B^b$ to construct the comparison matrix $\gamma^{ab}$. We then conduct hashing, obtain the compressed $\tilde{\gamma}^{ab}$ for later calculations, and delete the larger $\gamma^{ab}$ from memory before continuing with the next chunk of data. In detail, we calculate
%	\begin{subequations}
%		\begin{align}
%			r_{p_j}^{ab} &= \{i \in  | \gamma_{ij} = h_p, j \in B^b\}, \label{r_{p_j}^{ab}} \\
%			N_{p_j}^{ab} &= |r_{p_j}^{ab}|, \label{N_{p_j}^{ab}}
%		\end{align}
%	\end{subequations}
%	These can computed serially or in parallel. Summary statistics from each pairwise chunk comparison can be easily synthesized to recover summary statistics for the full comparison matrix $\gamma$. Specifically, we combine information through
%	\begin{subequations}
%		\begin{align}
%			r_{p_j} &= (r_{p_j}^{11}, \ldots, r_{p_j}^{ t_A t_B}) \text{ for } a = 1, \ldots, t_A \text{ and } b = 1, \ldots, t_B,\label{r_{p_j}} \\
%			N_{p_j} &= \sum_{a = 1}^{t-A} \sum_{b = 1}^{t_B} N_{p_j}^{ab},\label{N_{p_j}}	\end{align}
%	\end{subequations}
%	
%	
%	\hypertarget{app:fabl-psuedocode}{%
%		\subsection{Summary of Fast Beta Linkage Method}\label{app:fabl-psuedocode}}
%	
%	\textcolor{red}{I am getting rid of the notation $h(Z_j)$ for the agreement pattern between $B_j$ and its potential match. Instead, I am keeping notation that already exists, saying $(Z_j, j) \in h_p$. With this change, I don't see a good way to write lines 15 and 17. For that reason, I think I should remove this pseudo-code; it doesn't seem necessary, and invites criticism.}
%	
%	\begin{algorithm}[h!]
%		\begin{algorithmic}[1]
%			
%			\Procedure{Hashing and Preprocessing}{}
%			\State Construct and enumerate set of unique patterns $\mathcal{P}$ from $F$ and $\{L_f\}$.
%			\State Partition files $\bm{X_1}$ and $\bm{X_2}$ into chunks $\{\bm{X_{1n}}\}$, $\{\bm{X_{2m}}\}$.
%			\For{each $n$, $\bm{m}$}
%			\State Create comparison vectors between $\bm{X_{1n}}$ and $\bm{X_{2m}}$.
%			\State Hash records to $\mathcal{R}_{nm}$ and calculate summary statistics $\mathcal{H}_{nm}$.
%			\State Use SEI to reduce memory usage; $\mathcal{R}_{nm} \to \mathcal{R}_{nm}^{\text{SEI}}$.
%			\EndFor
%			\State Synthesize results across pairings to get $\tilde{\gamma} = \{\mathcal{P}, \mathcal{R}, \mathcal{H} \}$.
%			\EndProcedure
%			
%			\Procedure{Gibbs Sampling}{}
%			\State Initialize $\bm{m}$, $\bm{u}$, and $Z$ parameters.
%			
%			\For{$t \in \{1, \ldots, T\}$} 
%			\State Sample $\bm{m}^{t+1}|Z^{t}, \tilde{\gamma}$ and  $\bm{u}^{t+1}|\bm{Z}^{t}, \tilde{\gamma}$
%			\State Sample $h\left(\bm{Z}^{t+1}\right)|\bm{m}^{t+1}, \bm{u}^{t+1}, \tilde{\gamma}$.  \Comment{Sample agreement pattern, not record}
%			\EndFor
%			\State Sample $\bm{Z} | h(\bm{Z}), \tilde{\gamma}$. \Comment{Fills in record label based on agreement pattern}
%			
%			\EndProcedure
%			
%		\end{algorithmic}
%	\end{algorithm}
%	\clearpage
	
	\hypertarget{app:ohe}{%
		\subsection{One Hot Encoding Transformation}\label{app:ohe}}
	
	As described in Section \ref{sec:efficiency}, \texttt{fabl} makes use of one-hot encodings to aid in vectorized computations. For $\gamma_{ij}^f$ with $L_f$ levels, define $e_{ij}^f$ to be an $L_f \times 1$ vector.  When $\gamma_{ij}^f = l$, we set the $l^{th}$ element of $e_{ij}^f$ to be 1, and set the other $L_f - 1$ elements of $e_{ij}^f$ to be 0. We then concatenate the $e_{ij}^f$ for all $f \in \{1, \ldots, F\}$, resulting in the one-hot encoded comparison vector $e_{ij}$ of length $\sum_{f=1}^F L_f$.
	
	For example, consider comparing the toy records shown in Table \ref{tab:ohe} with $L = (3, 3, 2, 2)$ levels of agreement for last name, first name, DOB, and city respectively. Since the first name differs by only one letter, a reasonable comparison vector for this pair would be $\gamma_{ij} = (1, 2, 1, 2)$. The one hot encoding representation of this vector is $e_{ij} = (1, 0, 0, 0, 1, 0, 1, 0, 0, 1)$. 
	\begin{table}[h!]
		\centering
		\begin{tabular}[h!]{llll}
			Last Name & First Name & DOB & City \\
			\hline
			Smith & Taylor & 01/01/2000 & Durham\\
			Smith & Tayler & 01/01/2000 & Raleigh\\
			\hline
		\end{tabular}
		\caption{Example records for one hot encoding.}\label{tab:ohe}
	\end{table}

	\hypertarget{partial}{%
	\subsection{Accuracy under Partial Estimates}\label{partial}}

In this section, we repeat the simulation study in Section \ref{accuracy} of the main text, allowing for clerical review rather than forcing all records to have or not have links.  Specifically, by leaving $\theta_{10} = \theta_{01} = 1$ and $\theta_{11} = 2$, but setting $\theta_R = 0.1$, we allow the model to decline to decide a match for certain records, with nonassignment being 10\% as costly as a false match. In this context, we are no longer focused on finding all true matches, but rather protecting against false matches. Thus, instead of recall, we use the negative predictive value (NPV), defined as the proportion of non-links that are actual non-matches. Mathematically, $\text{NPV} = \sum_{j=1}^{n_B} I(\hat{Z}_j = Z_j = n_A + j)$/$\sum_{j=1}^{n_B} I(\hat{Z}_j = n_A + j)$. We continue to use the precision, which is renamed the positive predictive value (PPV) in this context. Lastly, we also examine the rejection rate (RR), or how often the model declines to make a linkage decision, defined as RR = $\sum_{j=1}^{n_B} I(\hat{Z}_j = R)/n_B$. To convey this information alongside NPV and PPV, for which values close to 1 indicate strong performance, we report the decision rate (DR), defined as DR = $1 - RR$.

In Figure \ref{fig:sadinle_simulation_partial}, we see that \texttt{fabl} maintains equivalently strong PPV as \texttt{BRL} across all linkage settings. However, with high amounts of error, and thus fewer accurate and discerning fields of information, the rejection rate under \texttt{fabl} rises, leading to a decrease in NPV. Since \texttt{fabl} does not remove previously matched records from consideration for a new record, posterior probabilities of matches at times can be split across more records; in contrast, \texttt{BRL} is able to maintain higher confidence in matches in this setting. If one wishes to use partial estimates, \texttt{fabl} will possibly leave more linkages for the modeller to match by hand than would be left under \texttt{BRL}, but the decisions made by each method will have nearly equal accuracy. 

%In practice, it is rare to encounter linkage tasks with 90\% overlap, so we remain confident in \texttt{fabl}'s performance. If one wishes to use partial estimates, \texttt{fabl} will possibly leave more linkages for the modeller to match by hand that would be left under \texttt{BRL}, but the decisions made by each method will have nearly equal accuracy. 

%This is reasonable because large numbers of errors blur the distinction between matched and non-matched pairs, leading to situations where one record in $\bm{X}_2$ has multiple plausible matches in $\bm{X}_1$, so that posterior match probability is split across more records.

\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.6\textwidth]{../notes/figures/sadinle_sim_plot_partial_DR} 
		\caption{Negative predictive value (NPV), positive predictive value (PPV), and decision rate (DR) on data files in the simulation in Appendix \ref{partial}. We see poorer performance for \texttt{fabl} only in situations with high overlap.}
		\label{fig:sadinle_simulation_partial}
	\end{center}
\end{figure}
	
	
	\hypertarget{appendix-sim}{%
		\subsection{Traceplots for Simulation Study}\label{app:appendix-sim}}
	Below are traceplots for one of the 900 linkage tasks that comprise the simulation in Section \ref{accuracy}. It is set up with one error across the linkage fields and 50 duplicates across files. Traceplots across other settings exhibit similar behavior. Note that traceplots for $\bm{u}$ parameters show very little variation because the overwhelming majority of record pairs are nonmatching.  
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/sim_overlap_trace} 
			\caption{Representative traceplot of overlap between files from simulation study in Section \ref{accuracy}.}\label{fig:sim_overlap_trace}
		\end{center}
	\end{figure}
	
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/sim_m_trace} 
			\caption{Representative traceplot of $\bm{m}$ parameter from simulation study in Section \ref{accuracy}.}\label{fig:sim_m_trace}
		\end{center}
	\end{figure}
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/sim_u_trace} 
			\caption{Representative traceplot of $\bm{u}$ parameters from simulation study in Section \ref{accuracy}.}\label{fig:sim_u_trace}
		\end{center}
	\end{figure}

\clearpage

	\hypertarget{appendix-speed}{%
	\subsection{Additional Speed Simulation Study}\label{app:appendix-speed}}
	To illustrate that different constructions of the comparison vectors lead to similar speed gains, we replicate the speed study of Section \ref{speed} under different settings. Here, we use four fields of comparison, each with three possible levels of agreement, resulting in $3^4 = 81$ possible patterns. The $\bm{m}$ and $\bm{u}$ parameters for this simulation are shown Table \ref{Tab:distributions_2}.
	
		\begin{table}[t]
		\centering
		\begin{tabular}{llll|lll}
			\multicolumn{1}{c}{ } & \multicolumn{3}{c}{$\bm{m}$} & \multicolumn{3}{c}{$\bm{u}$} \\
			\cline{2-4} \cline{5-7}
			& Agree & Partial & Disagree & Agree & Partial & Disagree \\
			\hline
			Feature 1 & $\frac{9}{10}$ & $\frac{9}{100}$  & $\frac{1}{100}$ & $\frac{1}{100}$ &  $\frac{3}{100}$ & $\frac{96}{100}$ \\ 
			Feature 2 & $\frac{9}{10}$ & $\frac{9}{100}$  & $\frac{1}{100}$ & $\frac{1}{100}$ &  $\frac{3}{100}$ & $\frac{96}{100}$ \\ 
			Feature 3 & $\frac{9}{10}$ & $\frac{9}{100}$  & $\frac{1}{100}$ & $\frac{1}{100}$ &  $\frac{3}{100}$ & $\frac{96}{100}$ \\ 
			Feature 4 & $\frac{9}{10}$ & $\frac{9}{100}$  & $\frac{1}{100}$ & $\frac{1}{100}$ &  $\frac{3}{100}$ & $\frac{96}{100}$ \\ 
			\hline
		\end{tabular}
		\caption{Probabilities used for $\bm{m}$ and $\bm{u}$ distributions in simulation study in Appendix \ref{app:appendix-speed}.}\label{Tab:distributions_2}
	\end{table}
	
	\begin{figure}[h!]
		\begin{center} \includegraphics[width=0.6\textwidth]{../notes/figures/sadinle_speed_plot_slides2} 
			\caption{Run-time for \texttt{BRL} and \texttt{fabl} to run 1000 Gibbs iterations in simulation study in Appendix \ref{app:appendix-speed}, including hashing step for \texttt{fabl}, for increasing values of both $n_A$ and $n_B$. We see near quadratic growth in run-time for \texttt{BRL}, and near linear growth for \texttt{fabl}.}\label{fig:app-speed1}
		\end{center}
	\end{figure}
	
	\begin{figure}[h!]
		\begin{center} \includegraphics[width=0.6\textwidth]{../notes/figures/speed_plot_fixed_nB_slides2} 
			\caption{Run-time for \texttt{BRL} and \texttt{fabl} to run 1000 Gibbs iterations in simulation study in Appendix \ref{app:appendix-speed}, including hashing step for \texttt{fabl}, with increasing $n_A$ and $n_B$ fixed at 500. We see linear growth in run-time for \texttt{BRL}, and near constant run-time for \texttt{fabl}.}\label{fig:app-speed2}
		\end{center}
	\end{figure}

\clearpage
	
	\hypertarget{appendix-es}{%
		\subsection{Traceplots for El Salvador Case Study}\label{app:appendix-es}}
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/overlap_trace} 
			\caption{Traceplot for number of matches found across data files in El Salvador case study.} \label{fig:overlap_trace}
		\end{center}
	\end{figure}
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/m_trace} 
			\caption{Traceplot for $\bm{m}$ parameter in El Salvador case study.} \label{fig:m_trace}
		\end{center}
	\end{figure}
	
	\begin{figure}[!h]
		\begin{center}
			\includegraphics[width=0.6\textwidth]{../notes/figures/el_salvador/u_trace} 
			\caption{Traceplot for $\bm{u}$ parameter in El Salvador case study.} \label{fig:u_trace}
		\end{center}
	\end{figure}
\end{document}
