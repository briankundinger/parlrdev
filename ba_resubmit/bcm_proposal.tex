\documentclass{beamer}
\usetheme{Rochester}
\addtobeamertemplate{navigation symbols}{}{ \hspace{1em}    \usebeamerfont{footline}%
	\insertframenumber / \inserttotalframenumber }
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	pdftitle={Overleaf Example},
	pdfpagemode=FullScreen,
}

\begin{document}
	\title{Joint Matching and Treatment Effect Estimation in Causal Inference through Bayesian Fellegi-Sunter Methods}
	%\subtitle{Using Beamer}
	\author{Brian Kundinger}
	\institute{Duke University}
	\date{\today}
	
%	\AtBeginSection[]{
%		\begin{frame}
%			\vfill
%			\centering
%			\begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
%				\usebeamerfont{title}\insertsectionhead\par%
%			\end{beamercolorbox}
%			\vfill
%		\end{frame}
%	}

\AtBeginSection[]
{
	\begin{frame}
		\frametitle{Table of Contents}
		\tableofcontents[currentsection]
	\end{frame}
}
	
	\begin{frame}
		\titlepage
	\end{frame}

\section{Matching in Causal Inference}
\begin{frame}{Overview of Approaches}
	\begin{itemize}
		\item Exact Matching
		\item Mahalonobis Distance
		\item Propensity Score
		\item Almost Exact Matching
	\end{itemize}
\end{frame}

\begin{frame}{Mahalonobis Distance and Propensity Matching}
	\begin{itemize}
	\item One approach to matching is to use the Mahalonobis distance, which measures distance between units purely in the covariate space, without regard to which features are related to the outcome of interest. It is defined by
		\begin{align}
			d(X_1, X_2) = \sqrt{|X_1 - X_2| S^{-1} |X_1 - X_2|}
		\end{align}
	\item Another method is to estimate propensity scores $p(Z_n = 1 |X_n)$ for each unit in the data, and match units based on this score.
	\item Most matching methods in causal inference then use deterministic algorithms to match treated units to their nearest control unit based on the chosen metric
	\end{itemize}
\end{frame}

\begin{frame}{Pitfalls of Both Approaches}
	\begin{itemize}
		\item The pros and cons of each method are summarized very well on this \href{https://stats.stackexchange.com/questions/511294/what-are-the-pros-and-cons-of-using-mahalanobis-distance-instead-of-propensity-s}{Stack Exchange} page
		\item Essentially, propensity scores can end up matching units with that have similar probability of treatment, but have very different covariate values
		\item Conversely, Mahalonobis can end up matching units that are similar on features that are irrelevant to the outcome of interest
	\end{itemize}
\end{frame}

\begin{frame}{Almost Exact Matching (AME)}
	\begin{itemize}
		\item I am new to these readings, but it seems like most of them
		\begin{itemize}
			\item Learn a scoring function (or equivalently, weights on individuals covariates)
			\item Apply scoring function to all units, and find units 
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Additional Thoughts}
	\begin{itemize}
		\item  \href{https://www.jstatsoft.org/article/view/v042i07}{Sekhot (2011)} recommends including the propensity score into the Malahalonobis distance. This is just practical advice, but it forms the foundation of thoughts to come
		\item \href{https://www.lexjansen.com/pharmasug/2006/PublicHealthResearch/PR05.pdf}{This paper} estimates PS, identifies all units in the treatment have PS within a specified caliper, and then chooses the unit with the smallest Mahalanobis distance. No uncertainty quantification
		\item \href{https://arxiv.org/pdf/2105.02362.pdf}{Alvarez and Levin (2021)} (Section 2, pages 5 - 7) use a Bayesian model to create draws of the propensity scores. For set of scores, they use a deterministic algorithm to match records. They do not use any other covariates
	\end{itemize}
\end{frame}

\section{New Research Idea}

\begin{frame}{Intuition}
	\begin{itemize}
		\item The machinery of Fellegi Sunter record linkage is very good at identifying duplicate records. It should also be good at identifying separate individuals that are similar.
		\item FS does matching purely on the reliability and uniqueness ($m$ and $u$ parameters) of fields of information. It does not take into account any relationship with an outcome variable. Conceptually, this is very similar to the Mahalonobis distance, but allows for uncertainty quantification.
		\item If we include propensity scores into FS, the model will place high weight on units with ``similar" scores. Then, the FS machinery for the other fields should find the units that are most similar. 
	\end{itemize}
\end{frame}

\begin{frame}{Nuts and Bolts}
	\begin{itemize}
		\item Data is $\{y_n, z_n, x_n\}_{n = 1}^N$ where $y_n$ is the outcome, $z_n$ is the treatment indicator, and $x_n$ is a vector of $F$ fields of covariates. 
		\item Split data into a datasets $T$ with records $i \in \{1, \ldots, n_T\}$ for treated units and $C$ with records $j \in \{1, \ldots, n_C\}$. 
		\item At each step of the Gibbs Sampler, calculate propensity score $PS(n) = P(Z_n = 1 | X)$ for each data point (logistic or probit regression)
	\end{itemize}
\end{frame}

\begin{frame}{Nuts and Bolts}
	\begin{itemize}
		\item Define comparison field 
		\begin{align*}
			\Gamma_{0} = \begin{cases}
				1 & $PS(i)$ \text{ and } $PS(j)$ \text{ are within specified caliper} \\
				0 & \text{otherwise} \\
			\end{cases}
		\end{align*} 
		\item Conduct Fellegi Sunter Matching on new full set of comparison vectors $\Gamma^{*} = (\Gamma_0, \gamma)$.
		Note $\gamma$ is fixed data, but the $\Gamma$ are random. 
	\end{itemize}
\end{frame}

\begin{frame}{Big Picture}
	\begin{itemize}
		\item Each iteration of the Gibbs sampler will provide a matched dataset, from which you can calculate an MCMC sample of the treatment effect (TE) or other quantities of interest
		\item We obtain measures of match quality as usual, through the posterior samples of the linkage structure
		\item I believe this to be first causal matching method to produce matchings probabilistically
		\item The proposed method is fully model based, with no deterministic algorithms. It propogates uncertainty between the matching procedure and the treatment effect estimation. 
	\end{itemize}
\end{frame}

\section{Proposed Models}

\begin{frame}{Model (Basic)}
	\begin{align*}
		Z_n &= x_n' \beta + \epsilon \\
		\beta &\sim N(0, \tau) \\
		\epsilon &\sim N(0, \sigma^2) \\
	\mathcal{L}(\Delta, m, u \mid \Gamma^{*}) &= \prod_{i=1}^{n_T}  \prod_{j=1}^{n_C}\prod_{f=0}^{F}\prod_{l=1}^{L_f}\left[  m_{fl}^{\Delta_{ij} = 1}u_{fl}^{\Delta_{ij} = 0}\right]^{I(\gamma_{ij}^f = l)} \\
		m, u &\sim \text{Dirichlet} \\
		\Delta &\sim \text{Desired Record Linkage Prior}
	\end{align*}
Different record linkage priors can induce one-to-one, many-to-one, and many-to-many matches. Any one of them can work within this framework. 
\end{frame}

\begin{frame}{Model (With Variable Selection)}
	\begin{itemize}
		\item In causal matching, it is especially important not to match on irrelevant features. Therefore, we can use a spike and slab prior on $\beta$, and only use fields where $\beta_f \neq 0$ in the linkage.
	\end{itemize}
\begin{align*}
	\beta_j & \sim \pi_0 \delta_0 + (1 - \pi_0)N(0, \tau_j) \\
	\mathcal{L}(\Delta, m, u \mid \Gamma^{*}, \beta) &= 
	\prod_{i=1}^{n_T}  \prod_{j=1}^{n_C}\prod_{f=0}^{F}\left[\prod_{l=1}^{L_f}\left[  m_{fl}^{\Delta_{ij} = 1}u_{fl}^{1 - \Delta_{ij} = 1}\right]^{I(\gamma_{ij}^f = l)}\right]^{\beta_f \neq 0} \\
\end{align*}
\end{frame}

\begin{frame}{Full Conditional for $\beta$}
	Reexpress the likelihood as
		\begin{align*}
			\mathcal{L}(\Delta, m, u, \beta \mid \Gamma^{*}) &= 
			\prod_{i=1}^{n_T}  \prod_{j=1}^{n_C}\prod_{f=0}^{F} c_f \prod_{l=1}^{L_f}\left(  \frac{m_{fl}}{u_{fl}}\right)^{I(\gamma_{ij}^f = l)I(\Delta_{ij} = 1)I(\beta_f \neq 0)} \\
			&= \prod_{i=1}^{n_T}  \prod_{j=1}^{n_C}\prod_{f=0}^{F} c_f \prod_{l=1}^{L_f}w_{fl}^{I(\gamma_{ij}^f = l)I(\Delta_{ij} = 1)I(\beta_f \neq 0)}
		\end{align*}
where $w_{fl} =  \frac{m_{fl}}{u_{fl}}$. Let $M_{fl} = \sum_{i, j} I(\Delta_{ij} = 1)I(\gamma_{ij}^f = l)$.

Then, conditional on $\Delta$, we have 
	\begin{align*}
	\mathcal{L}(m, u, \beta \mid \Gamma^{*}, \Delta) &= \prod_{i=1}^{n_T}  \prod_{j=1}^{n_C}\prod_{f=0}^{F} \left[c_f \prod_{l=1}^{L_f}w_{fl}^{M_{fl}}\right]^{I(\beta_f \neq 0)}
\end{align*}
\end{frame}

\begin{frame}{Full Conditional for $\beta$}
	The quantity $\prod_{l=1}^{L_f} w_{fl}^{M_{fl}}$ is measure of how strong field $f$ is in the linking procedure. And it shows up in the full conditional for $\beta_f$, showing how the regression and matching procedure are linked. 
\end{frame}

\section{Conclusion}

\begin{frame}{Caveats}
	\begin{itemize}
		\item Using a comparison vector made from the propensity score may technically violate the conditional independence assumption of FS
%		\item I can't imagine this idea totally failing, but I'm sure that it won't be cutting edge. Sekhot further devised a method that estimates
%		\begin{align}
%			d(X_1, X_2) = \sqrt{|X_1 - X_2| S^{-1/2} W S^{-1/2} |X_1 - X_2|}
%		\end{align}
%		where the entries in $W$ are fine tuned through optimization methods that encourage covariate balance across datasets, based on t-tests and all sorts of other measures. This is almost certainly outperform what I have presented, because it is able to use more information.
		\item Propensity and Mahalonobis matching both have theoretic garuntees. I don't know right now if this method would have them
		\item I know very little about causal inference! This would be a large undertaking
	\end{itemize}
\end{frame}

\begin{frame}{Questions}
	\begin{itemize}
		\item What do you all think of this idea?
		\item Jerry, how well versed are you in matching for causal inference? To what extent to should I seek guidance/collaboration from Fan Li or Alex Volfovsky?
	\end{itemize}
\end{frame}

\begin{frame}{Possible Theoretical Question}
	\begin{itemize}
		\item I'm really struck by the conceptual similarity between Mahalanobis distance and the FS weights. If you make some assumptions (categorical data, one-to-one matching, known parameters), it might be possible to prove a correspondence between the two. 
	\end{itemize}
\end{frame}

\begin{frame}{Possibly Better Approach}
	\begin{itemize}
		\item The FS model assumes that agreement levels are independent conditional on the matching status of the ``record" pair. This is often violated, with minimal issues. 
		
		However, the propensity score is very much not independent from other agreement levels. It may therefore to incorporate proximity in PS not as a feature, but rather as a \emph{latent class} for the agreement vector. 
		
		See \href{https://projecteuclid.org/journals/annals-of-applied-statistics/volume-13/issue-3/Incorporating-conditional-dependence-in-latent-class-models-for-probabilistic-record/10.1214/19-AOAS1256.full}{Xu et. al (2019)} for more on this. 
		
		I think the first method would work well, and presents enough innovation to stand on its own. I can do this latent class stuff in an extension. 
	\end{itemize}
\end{frame}


\end{document}