\documentclass{article}
\usepackage[numbers]{natbib}
%\usepackage{hyperref}
\usepackage{lineno}
\usepackage{amsthm}
\usepackage{amsmath, color}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
%\usepackage{graphicx}
%\newtheorem{lemma}{Lemma}
\usepackage{booktabs}       % professional-quality tables
\usepackage{algorithm}      % algorithm environment
\usepackage{algpseudocode}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{bm}
\usepackage{natbib}

\newcommand{\bam}{\boldsymbol{m}}    
\newcommand{\bu}{\boldsymbol{u}}    
\newcommand{\bM}{\boldsymbol{M}}  
\newcommand{\bU}{\boldsymbol{U}}    
\newcommand{\bZ}{\boldsymbol{Z}}  
\newcommand{\bX}{\boldsymbol{X}}
\newcommand{\bDelta}{\boldsymbol{\Delta}}
\newcommand{\bGamma}{\boldsymbol{\Gamma}}
\newcommand{\bgamma}{\boldsymbol{\gamma}}

%\newcommand{\bX}{\boldsymbol X}
%\newcommand{\bZ}{ \boldsymbol Z}
\newcommand{\bh}{\boldsymbol h}
\newcommand{\bc}{\boldsymbol c}
\newcommand{\btheta}{\boldsymbol \theta}
\newcommand{\LL}{\mathcal{L}} 
\newcommand{\C}{\mathcal{C}} 
\newcommand{\R}{\mathbb{R}} 
\newcommand{\CC}{\mathfrak{C}} 
\newcommand{\PP}{\mathbb P} 
\newcommand{\pairP}{\mathcal{P}}

\begin{document}
\title{AE Comments}
\author{}
\date{}
\maketitle




\section{AE Comments}

\textcolor{red}{Find some proposed responses to the reviewer for some of the comments based upon the May 11 in person meeting.}

\begin{enumerate}
\item Line 111. Is this missing at random or something else? This needs to be clarified (AE Comment 1). This needs to be addressed with extreme care regarding the precise wording. \\
\textcolor{blue}{This distinction between truly missing at random and missing completely at random does not matter due to two assumptions. Following Sadinle (2017), we assume that the fields in the comparison vectors are conditionally independent given the coreference matrix and we assume that the missing comparisons are ignorable (Sadinle (2017). Therefore, our likelihood is the same as in Sadinle (2017) (see equations (3) and (4) of his paper). As stated by Sadinle (2017), ``we work under the assumption of ignorability of the missingness mechanism for the comparisons so that we can base our inferences on the marginal distribution of the observed comparisons (Little and Rubin, p. 90)."  We refer to Sadinle (2017), Section 4.2, page 604 for further details.}
\item Appendix 8.2: I appreciate the streamlining of the discussion of the proposed algorithms and
	think it was a good choice to move the full derivation to an appendix. However, I do not follow
	the re-expression of the pmf for $\Gamma_{.j}$. First, the opening square brackets are still misplaced
	throughout. In the second line of this derivation, the authors divide the expression by a product
	of the element-wise conditional probabilities of a match in the comparison vector (u), raised by
	an indicator that the element of the comparison vector equals a particular value. Perhaps I
	misunderstand, but I believe this product is not constant in $\Gamma_{.j}$ or $u$, and so the total expression
	is not proportional to the line above, as interpreted as a pmf. I think that the end effect is that
	the last line of the newly expressed pmf is missing a factor of \\
%	$\prod_i \prod_f \prod_l u_{fl}^{I(\gamma_{ij}^f = l) I_{obs}$\\
	\textcolor{blue}{Is the new proof, Brian, missing an indicator over the cluster assignment? I believe that it should match those 
	of Sadinle (2017), Aleshin and Guendel (2022), and Wortman (2019). Where is the indicator regarding the cluster assignment that you condition on? Specifically, the update for each gamma will depend upon your cluster assignment, so I believe there is a typo for this reason. (To help with notation,  Jody calls this $S_k$, Sadinle calls it $Z_{-j}$, and Serge calls it $\C_{-j}$). Could you carefully check this please? I think this question is related to the next point/comment by the AE.} \\
	
	\textcolor{blue}{Brian: please re-derive these very carefully as I have found typos in the appendix after going back through them.}
    \item Appendix 8.2: Thank you for including the details on integrating out pi from the full
conditionals. They surprised me. I had assumed that the authors had integrated out pi in the
prior for Z, as this is what Sadinle (2017) had done to form the “beta prior for bipartite
matchings”. Does the alternate approach presented in this paper provide a different algorithm
than directly integrating the prior distribution? Is the presented alternative approach justified? \\
\textcolor{blue}{
Thank you for noting your surprise regarding our proposal. We do not extend the approach of Sadinle (2017), but relax the one-to-one assumption for computational reasons. This motivates the choice of our prior (instead of the beta prior for bipartite matchings). 
\begin{enumerate}
\item Both approaches that were previously provided in the revision are justified, however, did contain typos. The first one is providing a standard update of the partition or coreference matrix. The second provides the update to the partition or coreference matrix, where $\pi$ is integrated out for a computational benefit. Furthermore, when integrating out $\pi$ from the full conditional of the partition, there are connections to the original derivation the full conditional of Sadinle (2017). Specifically, each update to the full conditional is similar in nature to the original derivation of Sadinle (2017), where the main difference is that we no longer enforce the bipartite matching constraint. [TODO: Make this connection more clear in the revision].
\item Yes, the presented approach is justified as it is simply a relaxation of Sadline (2017). We utilize a Gibbs update in the same spirit of Sadinle (2017) and the rest of the record linkage literature. We provide details regarding both Gibbs samplers below.
\end{enumerate}
}

\newpage
\textbf{Gibbs sampler}

We present a Gibbs sampler to explore the joint posterior distribution of $\bZ$, $\Phi$, and $\pi$ given the observed comparison data $\bgamma_{\text{obs}},$ for the likelihood and priors presented earlier. 
We start the sampler with an empty bipartite matching, such that, $Z_{j}^{[0]} = n_1 + j$ for all $j \in \{1, \ldots, n_2 \}.$ For a current value of the matching labeling $\bZ^{[t]},$ we obtain the next values of the Gibbs sampler iteration 
$\bam_f^{[t+1]} = (m_{f0}^{[t+1]}, \ldots, m_{f L_f}^{[t+1]})$, 
$\bu_f^{[t+1]} =  (u_{f0}^{[t+1]}, \ldots, u_{f L_f}^{[t+1]}),$ for $f=1, \ldots, F,$ 
$\pi^{[t+1]}$, and $\bZ^{(t+1)} = (Z_1^{(t+1)}, \ldots, Z_{n2}^{(t+1)})$ as follows:

\begin{enumerate}
\item For $f=1, \ldots, F$ sample
$$\bam_f^{[t+1]} \mid \bgamma_{\text{obs}}, \bZ^{[t]} \sim
\text{Dirichlet}(
a_{f0}(\bZ^{(t)})  + \alpha_{f0}, \ldots, a_{f L_f}(\bZ^{(t)} ) + \alpha_{f L_f})
),
$$
and
$$\bu_f^{[t+1]} \mid \bgamma_{\text{obs}}, \bZ^{[t]} \sim
\text{Dirichlet}(
b_{f0}(\bZ^{(t)})  + \beta_{f0}, \ldots, b_{f L_f}(\bZ^{(t)} ) + \beta_{f L_f})
). 
$$
Collect these updated parameters into $\Phi^{[t+1]},$ where the functions $a_{f \ell}(\cdot)$ and $b_{f \ell}(\cdot)$ were defined in equation \ref{eqn:ab}. 
\item Sample $$\pi^{[t+1]} \
\sim \text{Beta}(
n_{12}(\bZ^{(t)} )+ \alpha_{\pi}, n_2 - n_{12}(\bZ^{(t)}) + \beta_{\pi} - 1)
)
$$
\item Sample the entries of $\bZ^{[t+1]}$ sequentially. As we have sampled the first $j-1$ entries of $\bZ^{[t+1]},$ we define 
$\bZ_{-j}^{(t + (j-1)/n_2)} = (
Z_{1}^{[t+1]}, \ldots, Z_{j-1}^{[t+1]}, Z_{j+1}^{[t+1]}, \ldots, Z_{n_2}^{[t+1]}
).
$
We sample a new label $Z_{j}^{[t+1]}$ with probability of \textcolor{blue}{selecting label} $q \in \{1, \ldots, n_1, n_1 + j \}$ given by 

\begin{align}
P(\bZ_{-j} \mid \Phi, \bgamma_{obs}) \propto 
\begin{cases}
\frac{\pi}{n_1}\exp(w_{qj}(\Phi) )  & \text{if} \; q \leq n_1 \\
1- \pi
%  \frac{n_1}\frac{n_2}{n_{12}(\bZ_{-j}) + \beta_{\pi}}{n_{12}(\bZ_{-j}) + \alpha_{\pi}}
   &  \text{if} \; q = n_1 + j.
\end{cases}
\end{align}
\end{enumerate}

Observe that the update for the partition does not depend upon any sequential updates, which makes this sampler extremely fast to update compared with the collapsed sampler below.


We present a collapsed Gibbs sampler to explore the joint posterior distribution of $\bZ$, $\Phi$ given the observed comparison data 
$\bgamma_{\text{obs}},$ for the likelihood and priors presented earlier, where we collapse (or integrate out) $\pi$ in order to illustrate similarities with the original proposal of Sadinle (2017). 

\textbf{Collapsed Gibb sampler}
We start the sampler with an empty bipartite matching, such that, $Z_{j}^{[0]} = n_1 + j$ for all $j \in \{1, \ldots, n_2 \}.$ For a current value of the matching labeling $\bZ^{[t]},$ we obtain the next values of the Gibbs sampler iteration 
$\bam_f^{[t+1]} = (m_{f0}^{[t+1]}, \ldots, m_{f L_f}^{[t+1]})$, 
$\bu_f^{[t+1]} =  (u_{f0}^{[t+1]}, \ldots, u_{f L_f}^{[t+1]}),$ for $f=1, \ldots, F,$ and $\bZ^{(t+1)} = (Z_1^{(t+1)}, \ldots, Z_{n2}^{(t+1)})$ as follows:

\begin{enumerate}
\item For $f=1, \ldots, F$ sample
$$\bam_f^{[t+1]} \mid \bgamma_{\text{obs}}, \bZ^{[t]} \sim
\text{Dirichlet}(
a_{f0}(\bZ^{(t)})  + \alpha_{f0}, \ldots, a_{f L_f}(\bZ^{(t)} ) + \alpha_{f L_f})
),
$$
and
$$\bu_f^{[t+1]} \mid \bgamma_{\text{obs}}, \bZ^{[t]} \sim
\text{Dirichlet}(
b_{f0}(\bZ^{(t)})  + \beta_{f0}, \ldots, b_{f L_f}(\bZ^{(t)} ) + \beta_{f L_f})
). 
$$
Collect these updated parameters into $\Phi^{[t+1]},$ where the functions $a_{f \ell}(\cdot)$ and $b_{f \ell}(\cdot)$ were defined in equation \ref{eqn:ab}. 

\item Sample the entries of $\bZ^{[t+1]}$ sequentially. As we have sampled the first $j-1$ entries of $\bZ^{[t+1]},$ we define 
$\bZ_{-j}^{(t + (j-1)/n_2)} = (
Z_{1}^{[t+1]}, \ldots, Z_{j-1}^{[t+1]}, Z_{j+1}^{[t+1]}, \ldots, Z_{n_2}^{[t+1]}
).
$
We sample a new label $Z_{j}^{[t+1]}$ with probability of \textcolor{blue}{selecting label} $q \in \{1, \ldots, n_1, n_1 + j \}$ given by 

\begin{align}
P(\bZ_{-j} \mid \Phi, \bgamma_{obs}) \propto 
\begin{cases}
\exp(w_{qj}(\Phi) )  & \text{if} \; q \leq n_1 \\
n_1 \frac{n_2- n_{12}(\bZ_{-j}) + \beta_{\pi}}
{n_{12}(\bZ_{-j}) + \alpha_{\pi}
}
%  \frac{n_1}\frac{n_2}{n_{12}(\bZ_{-j}) + \beta_{\pi}}{n_{12}(\bZ_{-j}) + \alpha_{\pi}}
   &  \text{if} \; q = n_1 + j.
\end{cases}
\end{align}
\end{enumerate}

The Gibbs sampler above is more directly comparable with that of \cite{} (see the supplement), where in our collapsed sampler we have relaxed the one-to-one assumption (or rather the bipartite matching) regarding the update to update to the partition. For computational reasons, we correct for this in our post-processing stage. Observe that our weights $\exp(w_{qj}(\Phi))$ are the same as in the work of Sadinle (2017).

\end{enumerate}


\end{document}